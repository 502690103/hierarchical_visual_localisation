{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Model pipeline\n",
    "1. Find images with similar global descriptors\n",
    "2. Cluster by covisiblity\n",
    "3. Find local descriptors\n",
    "4. Match to SfM model\n",
    "5. Calculate pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports, Loading data\n",
    "Loading data into memory. This may take some minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import sqlite3\n",
    "#import read_model as rm\n",
    "from dataset_loaders.txt_to_db import read_database\n",
    "import nearpy\n",
    "import threading\n",
    "from pyquaternion import Quaternion\n",
    "import transforms3d.quaternions as txq\n",
    "\n",
    "from dataset_loaders.utils import load_image\n",
    "from dataset_loaders.pose_utils import quaternion_angular_error\n",
    "import models.netvlad_vd16_pitts30k_conv5_3_max_dag as netvlad\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotmat2qvec(R):\n",
    "    Rxx, Ryx, Rzx, Rxy, Ryy, Rzy, Rxz, Ryz, Rzz = R.flat\n",
    "    K = np.array([\n",
    "        [Rxx - Ryy - Rzz, 0, 0, 0],\n",
    "        [Ryx + Rxy, Ryy - Rxx - Rzz, 0, 0],\n",
    "        [Rzx + Rxz, Rzy + Ryz, Rzz - Rxx - Ryy, 0],\n",
    "        [Ryz - Rzy, Rzx - Rxz, Rxy - Ryx, Rxx + Ryy + Rzz]]) / 3.0\n",
    "    eigvals, eigvecs = np.linalg.eigh(K)\n",
    "    qvec = eigvecs[[3, 0, 1, 2], np.argmax(eigvals)]\n",
    "    if qvec[0] < 0:\n",
    "        qvec *= -1\n",
    "    return qvec\n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "    return np.array([\n",
    "        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],\n",
    "        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],\n",
    "        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "        1 - 2 * qvec[1]**2 - 2 * qvec[2]**2]])\n",
    "\n",
    "def colmap_image_to_pose(image):\n",
    "    im_T_w = np.eye(4)\n",
    "    im_T_w[:3, :3] = qvec2rotmat(image.qvec)\n",
    "    im_T_w[:3, 3] = image.tvec\n",
    "    w_T_im = np.linalg.inv(im_T_w)\n",
    "    return w_T_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cursor(name):\n",
    "    return sqlite3.connect(name).cursor()\n",
    "\n",
    "def descriptors_from_colmap_db(cursor, image_id):\n",
    "    #print('Select descriptors from image_id=%d'%image_id)\n",
    "    cursor.execute('SELECT cols, data FROM descriptors WHERE image_id=?;',(image_id,))\n",
    "    feature_dim, blob = next(cursor)\n",
    "    desc = np.frombuffer(blob, dtype=np.uint8).reshape(-1, feature_dim)\n",
    "    return desc\n",
    "\n",
    "\n",
    "def keypoints_from_colmap_db(cursor, image_id):\n",
    "    cursor.execute('SELECT cols, data FROM keypoints WHERE image_id=?;',(image_id,))\n",
    "    cols, blob = next(cursor)\n",
    "    kpts = np.frombuffer(blob, dtype=np.float32).reshape(-1, cols)\n",
    "    return kpts\n",
    "\n",
    "def get_kpts_desc(cursor, image_id):\n",
    "    image_id = int(image_id)\n",
    "    kpts = keypoints_from_colmap_db(cursor, image_id)[:, :2]\n",
    "    desc = descriptors_from_colmap_db(cursor, image_id)\n",
    "    return kpts, desc\n",
    "\n",
    "def get_img_id(cursor, img_name):\n",
    "    img_id, = next(cursor.execute('SELECT image_id FROM images WHERE name=\"%s\";'%img_name))\n",
    "    return img_id\n",
    "\n",
    "def get_img_id_dataset(cursor, dataset_id):\n",
    "    db_query_name = 'db/%d.jpg'%dataset.get_img_id(dataset_id)\n",
    "    return get_img_id(cursor, db_query_name)\n",
    "\n",
    "def kpts_to_cv(kpts, kpt_size=1.0):\n",
    "    cv_kpts = []\n",
    "    for i, kpt in enumerate(kpts):\n",
    "        cv_kpts.append(cv2.KeyPoint(x=kpt[0], y=kpt[1], _size=kpt_size))\n",
    "    return cv_kpts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "images, points3d = read_database()\n",
    "t = time.time() - t\n",
    "print('Loaded data in {:.2f} seconds'.format(t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "points3d = rm.read_points3D_text('data/points3D.txt')\n",
    "print('%d 3d points'%len(points3d))\n",
    "#cameras = rm.read_cameras_text('data/cameras.txt')\n",
    "#print('%d cameras'%len(cameras))\n",
    "images = rm.read_images_text('data/images.txt')\n",
    "print('%d images'%len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_img = lambda i: np.array(load_image('data/AachenDayNight/images_upright/{}'.format(images[i].name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_cursor = get_cursor('data/AachenDayNight/aachen.db')\n",
    "query_cursor = get_cursor('data/queries.db')\n",
    "\n",
    "kpt_size = 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Find for each image in dataset all other images that share common points\n",
    "\n",
    "Parameters:\n",
    "path: path to .nvm file\n",
    "threshold: Number of points that images at least need to share\n",
    "\n",
    "Returns:\n",
    "img_cluster: Dictionary {id: set of all images that share points with Image}\n",
    "points_of_img: Dictionary maps camera ids to point ids\n",
    "points: Position of points\n",
    "\"\"\"\n",
    "def read_colmap_file(path, threshold=2):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        num_points = int(lines[2].strip())\n",
    "        sift_lines = lines[num_points+4:]\n",
    "        #lines = lines[:num_points+3]\n",
    "        #lines[3:] = [x.strip().split(' ') for x in lines[3:]]\n",
    "        num_sifts = int(sift_lines[0])\n",
    "        sift_lines = [x.strip().split(' ') for x in sift_lines[1:]]\n",
    "        print('Read 3d model')\n",
    "        #points_per_img = {i:[] for i in range(num_points)}\n",
    "        \"\"\"\n",
    "        <Camera> = <File name> <focal length> <quaternion WXYZ> <camera center> <radial distortion> 0\n",
    "        <Point>  = <XYZ> <RGB> <number of measurements> <List of Measurements>\n",
    "        <Measurement> = <Image index> <Feature Index> <xy>\n",
    "        \"\"\"\n",
    "        img_cluster = {}     # camera_id : set of other cameras that share points\n",
    "        points = []          # 3d positions of points\n",
    "        points_rgb = []      # rgb values of points\n",
    "        measurements = {}    # camera_id : list of measurements(point_index, feat_id, xy)\n",
    "        points_of_img = {}   # camera_id : set of associated points\n",
    "        \n",
    "        ## Iterate over 3d points\n",
    "        for i in range(num_sifts):\n",
    "            if i % 500000 == 0:\n",
    "                print('%d/%d'%(i,num_sifts))\n",
    "            ids = []\n",
    "            line = sift_lines[i]\n",
    "            ## extract point info\n",
    "            xyz = np.array([float(x) for x in line[:3]], dtype=np.float32)\n",
    "            rgb = np.array([int(x) for x in line[3:6]], dtype=np.int)\n",
    "            points.append(xyz)\n",
    "            points_rgb.append(rgb)\n",
    "            ## only use measurements above threshold\n",
    "            num_imgs = int(line[6])\n",
    "            if num_imgs < threshold:\n",
    "                continue\n",
    "            ## iterate over measurements\n",
    "            for j in range(num_imgs):\n",
    "                camera_idx = int(line[7+j*4])\n",
    "                feat_idx = int(line[8+j*4])\n",
    "                xy = np.array([float(line[9+j*4]), float(line[10+j*4])], dtype=np.float32)\n",
    "                if camera_idx in measurements:\n",
    "                    measurements[camera_idx]['point_id'].append(i)\n",
    "                    measurements[camera_idx]['kpts'].append([xy])\n",
    "                    measurements[camera_idx]['feat_id'].append(feat_idx)                  \n",
    "                else:\n",
    "                    measurements[camera_idx] = {'point_id' : [i], 'kpts': [xy], 'feat_id' : [feat_idx]}\n",
    "                ids.append(camera_idx)\n",
    "                if camera_idx in points_of_img:\n",
    "                    points_of_img[camera_idx].add(i)\n",
    "                else:\n",
    "                    points_of_img[camera_idx] = {i}\n",
    "            for j in ids:\n",
    "                if j not in img_cluster:\n",
    "                    img_cluster[j] = set(ids)\n",
    "                else:\n",
    "                    img_cluster[j] |= set(ids)\n",
    "\n",
    "        points = np.vstack(points)\n",
    "        points_rgb = np.vstack(points_rgb)\n",
    "        for cam in measurements:\n",
    "            measurements[cam]['kpts'] = np.vstack(measurements[cam]['kpts'])\n",
    "        print('Done loading %d cameras and %d 3d points'%(len(img_cluster), points.shape[0]))\n",
    "        return img_cluster, points_of_img, points, points_rgb, measurements\n",
    "\n",
    "db_path = 'data/AachenDayNight/aachen_cvpr2018_db.nvm'\n",
    "threshold = 0 # How many points need to be at least shared between images\n",
    "img_cluster, points_of_img, points, points_rgb, measurements = read_colmap_file(db_path, threshold)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get all image sizes\n",
    "print('Loading all original image sizes (This may take some time)')\n",
    "img_sizes = []\n",
    "real_size_dataset = aachen.AachenDayNight('data/AachenDayNight', True, train_split=-1,seed=0,input_types='img', output_types=[], real=True, verbose=False)\n",
    "for i in range(len(real_size_dataset)):\n",
    "    (d,t) = real_size_dataset[i]\n",
    "    if i % 1000 == 0:\n",
    "        print('%d/%d'%(i, len(real_size_dataset)))\n",
    "    img_sizes.append(np.array(d.size))\n",
    "    #if i not in measurements:\n",
    "    #    measurements[i] = {'point_id' : [], 'kpts': np.array([])}\n",
    "img_sizes = np.vstack(img_sizes)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##create image clusters multithreading\n",
    "def _cluster_thread(p_ids, img_cluster): \n",
    "    for p_id in p_ids:\n",
    "        img_ids = set(points3d[p_id].image_ids)\n",
    "        for img_id in img_ids:\n",
    "            #if img_id in img_cluster:\n",
    "            img_cluster[img_id] |= set(img_ids)\n",
    "            #else:\n",
    "            #    img_cluster[img_id] = set(img_ids)\n",
    "t = time.time()\n",
    "img_cluster = {img: set() for img in images.keys()} \n",
    "num_t = 12\n",
    "keys = list(points3d.keys())\n",
    "piece = len(keys)//num_t\n",
    "ts = [*[threading.Thread(target=_cluster_thread, args=(keys[i*piece:(i+1)*piece],img_cluster)) for i in range(num_t-1)], threading.Thread(target=_cluster_thread, args=(keys[piece*(num_t-1):],img_cluster))]\n",
    "for thread in ts:\n",
    "    thread.start()\n",
    "for thread in ts:\n",
    "    thread.join()\n",
    "t = time.time() - t\n",
    "print('Found %d cluster in %d s'%(len(img_cluster), t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_stats(errors_trans, errors_rot):\n",
    "    num_high, num_medium, num_coarse = 0,0,0\n",
    "    for t, q in zip(errors_trans, errors_rot):\n",
    "        if t <= 0.5 and q <= 2.0:\n",
    "            num_high += 1\n",
    "        if t <= 1.0 and q <= 5.0:\n",
    "            num_medium += 1\n",
    "        if t <= 5.0 and q <= 10.0:\n",
    "            num_coarse += 1\n",
    "    per_high = float(num_high)/float(len(errors_trans))*100.0\n",
    "    per_medium = float(num_medium)/float(len(errors_trans))*100.0\n",
    "    per_coarse = float(num_coarse)/float(len(errors_trans))*100.0\n",
    "    return (per_high, per_medium, per_coarse)\n",
    "\n",
    "print('Percentage results\\t{:.1f} / {:.1f} / {:.1f}\\n'.format(*percentage_stats([0.1, .9, 4.5, 11.0],[1.0, 2.5, 5.5, 11.0])), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create image clusters\n",
    "t = time.time()\n",
    "img_cluster = {img : set() for img in images.keys()} \n",
    "for p_id in points3d.keys(): \n",
    "    img_ids = set(points3d[p_id].image_ids)\n",
    "    for img_id in img_ids:\n",
    "        #if img_id in img_cluster:\n",
    "        img_cluster[img_id] |= img_ids\n",
    "        #else:\n",
    "        #    img_cluster[img_id] = set(img_ids)\n",
    "t = time.time() - t\n",
    "print('Found %d cluster in %d s'%(len(img_cluster), t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera matrix\n",
    "camera_matrices = {}\n",
    "query_intrinsics_files = ['data/AachenDayNight/queries/day_time_queries_with_intrinsics.txt',\n",
    "                         'data/AachenDayNight/queries/night_time_queries_with_intrinsics.txt',\n",
    "                         'data/AachenDayNight/database_intrinsics.txt'\n",
    "                         ]\n",
    "for file_path in query_intrinsics_files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "        for line in lines:\n",
    "            # Format: `image_name SIMPLE_RADIAL w h f cx cy r`\n",
    "            line = line.split(' ')\n",
    "            img_path = line[0]\n",
    "            f = float(line[4])\n",
    "            cx = float(line[5])\n",
    "            cy = float(line[6])\n",
    "            rad_dist = float(line[7])\n",
    "            A = np.array([[f, 0, cx],[0, f, cy], [0, 0, 1]])\n",
    "            camera_matrices[img_path] = {'cameraMatrix': A, 'rad_dist':rad_dist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera_matrices['query/day/nexus5x/IMG_20161227_163307.jpg']\n",
    "print(len([cm.split('/')[-1] for cm in camera_matrices if 'query/day/nexus5x/' in cm]))\n",
    "print(len(camera_matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_resolution = 256\n",
    "n_images = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2 day time queries\n",
    "2 night time queries\n",
    "2 dataset queries\n",
    "\"\"\"\n",
    "dataset_queries = [1, 500, 250, 2500, 4000]\n",
    "path_to_queries = [\n",
    "                   'data/AachenDayNight/images_upright/query/day/nexus5x/IMG_20161227_162905.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/day/nexus5x/IMG_20161227_160713.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/night/nexus5x/IMG_20161227_172616.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/night/nexus5x/IMG_20161227_191152.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/db/1.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/db/500.jpg'\n",
    "                  ] + ['data/AachenDayNight/images_upright/db/'+str(i)+'.jpg' for i in dataset_queries]\n",
    "n_queries = len(path_to_queries)\n",
    "low_res_transform = transforms.Compose([transforms.Resize(global_resolution), transforms.CenterCrop(global_resolution)])\n",
    "query_imgs_high_res = [load_image(path) for path in path_to_queries]\n",
    "#query_imgs_low_res = [transform(img) for img in query_imgs_high_res]\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "plt.title('Queries')\n",
    "plt.axis('off')\n",
    "for i, img in enumerate(query_imgs_high_res):\n",
    "    a = fig.add_subplot(1, len(query_imgs_high_res), i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find similar images (global descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = netvlad.vd16_pitts30k_conv5_3_max_dag(weights_path='data/teacher_models/netvlad_pytorch/vd16_pitts30k_conv5_3_max_dag.pth')\n",
    "model.eval()\n",
    "query_global_desc = [model(transforms.ToTensor()(low_res_transform(img)).unsqueeze(0)).detach().cpu().squeeze(0).numpy() for img in query_imgs_high_res]\n",
    "query_global_desc = np.vstack(query_global_desc)\n",
    "print(query_global_desc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global descriptors for dataset are precalculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading database global features')\n",
    "t = time.time()\n",
    "global_features_cursor = get_cursor('data/global_features.db')\n",
    "global_features = []\n",
    "image_ids = []\n",
    "for row in global_features_cursor.execute('SELECT image_id, cols, data FROM global_features;'):\n",
    "    global_features.append(np.frombuffer(row[2], dtype=np.float32).reshape(-1, row[1]))\n",
    "    image_ids.append(row[0])\n",
    "global_features = np.vstack(global_features)\n",
    "print(global_features.shape)\n",
    "global_features_cursor.close()\n",
    "t = time.time() - t\n",
    "print('%d seconds'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_neighbor_method = 'LSH' #choices=['LSH', 'exact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "if global_neighbor_method is 'LSH':\n",
    "    buckets = 5\n",
    "    engine = nearpy.Engine(global_features.shape[1], lshashes=[nearpy.hashes.RandomBinaryProjections('rbp', buckets)],\n",
    "                          distance = nearpy.distances.EuclideanDistance())\n",
    "    counts = []\n",
    "    for i, v in enumerate(global_features):\n",
    "        engine.store_vector(v, '%d'%i)\n",
    "        counts.append(engine.candidate_count(v))\n",
    "    indices = [engine.neighbours(d) for d in query_global_desc]\n",
    "    indices = np.array([np.array([int(n[1]) for n in nbr])[:n_images] for nbr in indices])\n",
    "    counts = np.array(counts)\n",
    "    print('Candidate count max: {}\\nCandidate count min: {}\\nCandidate count avg: {}\\nCandidate count median: {}'.format(counts.max(), counts.min(), counts.mean(), np.median(counts)))\n",
    "    print('Optimal: {}'.format(global_features.shape[1]/float(2**buckets)))\n",
    "elif global_neighbor_method is 'exact':\n",
    "    print('Find nearest neighbors for queries')\n",
    "    print('%d data points with %d sized feature vectors'%(global_features.shape[0], global_features.shape[1]))\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_images).fit(global_features)\n",
    "    print('Fitted')\n",
    "    distances, indices = nbrs.kneighbors(query_global_desc)\n",
    "    global_features_cursor.close()\n",
    "t = time.time() - t\n",
    "print('%d seconds'%t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for nbr in indices:\n",
    "    print(len(nbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, query_img in enumerate(query_imgs_high_res):\n",
    "    imgs = [query_img] \n",
    "    imgs = imgs + [get_img(image_ids[j]) for j in indices[i]]\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    #plt.title('Neighbors')\n",
    "    for j, img in enumerate(imgs):\n",
    "        a = fig.add_subplot(n_queries, n_images+1, i*(n_images+1)+j+1)\n",
    "        plt.imshow(img)\n",
    "        if j > 0:\n",
    "            plt.title('Neighbor')#'%.0f'%distances[i][j-1])\n",
    "        else:\n",
    "            plt.title('Query')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covisibility clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert indices to colmap indices\n",
    "#indices_colmap = np.array([get_img_id_dataset(database_cursor, xii) for xi in indices for xii in xi]).reshape(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_query = [img_cluster[image_ids[indices[query_id][0]]]]\n",
    "cluster_orig_ids = [image_ids[indices[query_id][0]]]\n",
    "for i, ind in enumerate(indices[query_id]):\n",
    "    ind = image_ids[ind]\n",
    "    if i == 0:\n",
    "        continue\n",
    "    point_set = img_cluster[ind]\n",
    "    print('Match neighbor %d'%i)\n",
    "    disjoint = False\n",
    "    for j, c in enumerate(cluster_query):\n",
    "        if ind in c:\n",
    "            print('  - Can be matched to cluster')\n",
    "            #print(point_set - (point_set-cluster))\n",
    "            cluster_query[j] |= point_set\n",
    "            disjoint = True\n",
    "            break\n",
    "    if not disjoint:\n",
    "        print('  - New cluster created')\n",
    "        cluster_orig_ids.append(ind)\n",
    "        cluster_query.append(point_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs_per_cluster = 15\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "plt.title('Covisibility clustering')\n",
    "plt.axis('off')\n",
    "for i, cluster in enumerate(cluster_query):\n",
    "    imgs = list(cluster_query[i])[:num_imgs_per_cluster]\n",
    "    imgs = [get_img(cluster_orig_ids[i])]+[get_img(j) for j in imgs]\n",
    "    for j, img in enumerate(imgs):\n",
    "        a = fig.add_subplot(len(cluster_query), len(imgs), i*len(imgs)+j+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        if j == 0:\n",
    "            plt.title('Clustered by')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_points = []\n",
    "for i, c in enumerate(cluster_query):\n",
    "    imgs_cluster = list(c)\n",
    "    points_cluster = set()\n",
    "    for ind in imgs_cluster:\n",
    "        pt_ids = images[ind].point3D_ids\n",
    "        valid = pt_ids > 0\n",
    "        points_cluster |= set(pt_ids[valid])\n",
    "    points_cluster = list(points_cluster)\n",
    "    cluster_points.append(np.stack([points3d[x].xyz for x in points_cluster]))\n",
    "    \"\"\"mask = np.ones(len(points3d),dtype=bool) #np.ones_like(a,dtype=bool)\n",
    "    mask[points_cluster] = False\n",
    "\n",
    "    cluster_points.append(points3d[~mask])\"\"\"\n",
    "    print('%d\\tpoints in cluster'%cluster_points[i].shape[0])\n",
    "#other_points = points[mask]\n",
    "#print('%d other points'%other_points.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 300\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.scatter3D(other_points[:,0], other_points[:,2], other_points[:,1], s = 0.5, alpha = 0.01)\n",
    "for cp in cluster_points:\n",
    "    ax.scatter3D(cp[:,0], cp[:,2], cp[:,1], s = 0.5, alpha = 0.25)\n",
    "median = np.sum([np.median(cp, axis=0)*cp.shape[0] for cp in cluster_points], axis=0)/float(sum([cp.shape[0] for cp in cluster_points]))\n",
    "print('Median is %s'%median)\n",
    "ax.set_xlim3d(median[0]-thresh,median[0]+thresh)#min(sift_points[:,0]),min(sift_points[:,0])+max_dist)\n",
    "ax.set_ylim3d(median[2]-thresh,median[2]+thresh)#min(sift_points[:,1]),min(sift_points[:,1])+max_dist)\n",
    "ax.set_zlim3d(median[1]-50, median[1]+50)#min(sift_points[:,2]),min(sift_points[:,2])+max_dist)\n",
    "ax.view_init(elev=35., azim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find local descriptors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(img_sizes.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "@param kpts: Numpy array (N, 2) with xy values of keypoints in image\n",
    "\"\"\"\n",
    "def resize_keypoints(kpts, img_dim, resize=256):\n",
    "    larger_axis = 0 if img_dim[0] > img_dim[1] else 1\n",
    "    larger_dim, smaller_dim = img_dim[larger_axis], img_dim[abs(1-larger_axis)]\n",
    "    empty_pixel = larger_dim-smaller_dim\n",
    "    lower_border = empty_pixel//2\n",
    "    upper_border = larger_dim - (empty_pixel // 2 + (empty_pixel % 2 > 0 ) )\n",
    "    valid =  [kpts[i,larger_axis] < upper_border and kpts[i,larger_axis] > lower_border for i in range(kpts.shape[0])]\n",
    "    if all(not x for x in valid):\n",
    "        return []\n",
    "    kpts = kpts[valid]\n",
    "    assert np.any(kpts[:,larger_axis] > lower_border), 'Lower border failed'\n",
    "    assert np.any(kpts[:,larger_axis] < upper_border), 'Upper border failed'\n",
    "    kpts[:,larger_axis] -= float(lower_border)\n",
    "    assert kpts.max() < smaller_dim, 'Crop gone wrong'\n",
    "    kpts *= float(resize-1)/float(smaller_dim)\n",
    "    return kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = cluster_orig_ids[0] #197"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Visualize resize of keypoints\n",
    "if 'orig_measurements' in locals():\n",
    "    kts_resized = kpts_0\n",
    "else:\n",
    "    kpts_0 = measurements[test_id]['kpts']\n",
    "    kpts_resized = resize_keypoints(kpts_0, img_sizes[test_id], resize=resolution)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(np.array(dataset[test_id][0]))\n",
    "if len(kpts_resized) > 0:\n",
    "    plt.scatter(kpts_resized[:,0], kpts_resized[:,1], s=0.25, alpha=0.9, c='red')\n",
    "plt.title('Resized/cropped image and keypoints')\n",
    "a = fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(np.array(real_size_dataset[test_id][0]), interpolation='bilinear')\n",
    "plt.scatter(kpts_0[:,0], kpts_0[:,1], s=0.25, alpha=0.9, c='red')\n",
    "plt.title('Original Image size and keypoints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not 'orig_measurements' in locals():\n",
    "    orig_measurements = measurements.copy()\n",
    "    for i, cam in enumerate(measurements):\n",
    "        if i % 1000 == 0:\n",
    "            print('%d/%d'%(i, len(measurements)))\n",
    "        measurements[cam]['kpts'] = resize_keypoints(measurements[cam]['kpts'], img_sizes[cam], resize=resolution)\n",
    "else:\n",
    "    print('already resized')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query_img = np.array(query_imgs_high_res[query_id])\n",
    "query_kpts, query_des = sift.detectAndCompute(query_img, None)\n",
    "query_sift_img = cv2.drawKeypoints(query_img, query_kpts, None)\n",
    "med_kpt_size = np.median([x.size for x in query_kpts])\n",
    "print('Median keypoint size: %.2f'%med_kpt_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Order to get image - keypoint - 3D point correspondances: </b>\n",
    "1. take image\n",
    "2. find image name\n",
    "3. find db_id of image_name\n",
    "4. get keypoints\n",
    "5. get valid 3d points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = cluster_orig_ids[0]\n",
    "db_id = test_id#get_img_id_dataset(database_cursor, test_id)\n",
    "img_name = images[db_id].name\n",
    "neighbor_img_original = np.array(load_image('data/AachenDayNight/images_upright/'+img_name))\n",
    "print(img_name)\n",
    "valid = images[db_id].point3D_ids > 0 \n",
    "#print([i for i in range(len(valid)) if valid[i]])\n",
    "neighbor_kpts, neighbor_desc = get_kpts_desc(database_cursor, db_id)\n",
    "neighbor_kpts = kpts_to_cv(neighbor_kpts[valid[:neighbor_kpts.shape[0]]] - 0.5)\n",
    "pt_ids = images[db_id].point3D_ids[valid]\n",
    "print('len kpts: %d'%len(neighbor_kpts))\n",
    "plt.figure(figsize=(10,10))\n",
    "neighbor_img = cv2.drawKeypoints(neighbor_img_original,neighbor_kpts, None)\n",
    "plt.imshow(neighbor_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "neighbor_desc = neighbor_desc[valid[:neighbor_desc.shape[0]]]\n",
    "#print(images[db_id])\n",
    "print(pt_ids.shape)\n",
    "print(len(neighbor_kpts))\n",
    "print(len(valid)//2)\n",
    "print(neighbor_desc.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img_id = get_img_id_dataset(database_cursor, test_id)\n",
    "neighbor_kpts, neighbor_desc = get_kpts_desc(database_cursor, img_id)\n",
    "neighbor_kpts = kpts_to_cv(neighbor_kpts)\n",
    "neighbor_img = cv2.drawKeypoints(np.array(dataset[test_id][0]),neighbor_kpts, None)\n",
    "plt.imshow(neighbor_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_id < 4:\n",
    "    test_query_path = path_to_queries[query_id].replace('data/AachenDayNight/images_upright/query/', '')\n",
    "    print(test_query_path)\n",
    "    query_img_id = get_img_id(query_cursor, test_query_path)\n",
    "    query_kpts, query_desc = get_kpts_desc(query_cursor, query_img_id)\n",
    "else:\n",
    "    fake_query_path = 'db/'+path_to_queries[query_id].split('/')[-1]\n",
    "    query_kpts, query_desc = get_kpts_desc(database_cursor, get_img_id(database_cursor,fake_query_path))\n",
    "    \n",
    "query_kpts = kpts_to_cv(query_kpts)\n",
    "query_img = cv2.drawKeypoints(np.array(query_imgs_high_res[query_id]),query_kpts, None)\n",
    "plt.imshow(query_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kpts = []\n",
    "#print(measurements[test_id]['kpts'])\n",
    "pts = measurements[test_id]['point_id']\n",
    "for i, kpt in enumerate(measurements[test_id]['kpts']):\n",
    "    kpts.append(cv2.KeyPoint(x=kpt[0], y=kpt[1], _size=med_kpt_size, _class_id=pts[i]))\n",
    "#print(len(kpts))\n",
    "kpts_sift, db_des = sift.compute(np.array(dataset[test_id][0]), kpts)\n",
    "dataset_sift_img=cv2.drawKeypoints(np.array(dataset[test_id][0]),kpts, None)\n",
    "dataset_sift_img_comp=cv2.drawKeypoints(np.array(dataset[test_id][0]),kpts_sift, None)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(dataset_sift_img)\n",
    "plt.axis('off')\n",
    "a = fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(dataset_sift_img_comp)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(query_img)\n",
    "plt.axis('off')\n",
    "plt.title('Query Image')\n",
    "a = fig.add_subplot(1,2,2)\n",
    "plt.imshow(neighbor_img)\n",
    "plt.axis('off')\n",
    "plt.title('Global neighbor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_vector_min = np.random.randint(0, 10, size=(3,4))\n",
    "print(test_vector_min)\n",
    "print(np.argmin(test_vector_min))\n",
    "print(np.argsort(test_vector_min)[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implemented Brute Force matching with numpy\n",
    "\"\"\"\n",
    "def own_matching_timing(query, data, ratio=.75, k=2, dist_method='L2'):\n",
    "    print('Start matching')\n",
    "    start_time = time.time()\n",
    "    print(data.shape)\n",
    "    print(query.shape)\n",
    "    if dist_method is 'L2':\n",
    "        subtract = np.array([np.square(data - q) for q in query])        \n",
    "    elif dist_method is 'L1':\n",
    "        subtract = np.array([np.abs(data - q) for q in query])\n",
    "    else:\n",
    "        raise NotImplementedError('dist method not implemented')\n",
    "    #subtract = np.array([np.linalg.norm(data - q) for q in query])\n",
    "    print(subtract.shape)\n",
    "    t = time.time() - start_time\n",
    "    print('Subtract takes \\t{:.2f}'.format(t))\n",
    "    t = time.time()\n",
    "    dist = np.sum(subtract, axis=2)\n",
    "    print('Sum takes \\t{:.2f}'.format(time.time()-t))\n",
    "    #min_id = [np.unravel_index(argmin, dist.shape) for argmin in np.argsort(dist.flatten())[:k]] ##gives back (query_id, data_id)\n",
    "    t = time.time()\n",
    "    matches = np.argsort(dist)[:,:2]\n",
    "    t = time.time() - t\n",
    "    print('Argsort takes \\t{:.2f}'.format(t))\n",
    "    t = time.time()\n",
    "    good_matches = []\n",
    "    for i, (a, b) in enumerate(matches): ## i: query index | a,b: data indices\n",
    "        if dist[i,b] > ratio*dist[i, a]:\n",
    "            good_matches.append([i,a])\n",
    "    t = time.time() - t\n",
    "    print('Ratio takes \\t{:.2f}'.format(t))    \n",
    "    total_time = time.time() - start_time\n",
    "    print('Total time: \\t{:.2f}'.format(total_time))\n",
    "    return good_matches\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def own_matches_to_cv2dmatch(matches):\n",
    "    return [cv2.DMatch(_queryIdx=m[0], _trainIdx=m[1], _imgIdx=0, _distance=0.0) for m in matches]\n",
    "\n",
    "#matches = own_matching(test_vector_q, test_vector_d)\n",
    "## time comparison:\n",
    "\"\"\"print('L2')\n",
    "matches_ownl2 = own_matching_timing(query_desc, neighbor_desc)\n",
    "print('L1')\n",
    "matches_ownl1 = own_matching_timing(query_desc, neighbor_desc, dist_method='L1')\n",
    "t = time.time()\n",
    "matcher = cv2.BFMatcher.create(cv2.NORM_L2)\n",
    "matches_bf = matcher.knnMatch(neighbor_desc, query_desc, k=2)\n",
    "good = []\n",
    "for i,(m,n) in enumerate(matches_bf):\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        #matchesMask[i]=[1,0]\n",
    "        good.append(m)\n",
    "t = time.time() - t\n",
    "print('OpenCV matcher: {:.2f}'.format(t))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_matching(query, data, ratio=.75, k=2, dist_method='L2'):\n",
    "    if dist_method is 'L2':\n",
    "        subtract = np.array([np.square(data - q) for q in query])        \n",
    "    elif dist_method is 'L1':\n",
    "        subtract = np.array([np.abs(data - q) for q in query])\n",
    "    else:\n",
    "        raise NotImplementedError('dist method not implemented')\n",
    "    dist = np.sum(subtract, axis=2)\n",
    "    matches = np.argsort(dist)[:,:2]\n",
    "    good_matches = []\n",
    "    for i, (a, b) in enumerate(matches): ## i: query index | a,b: data indices\n",
    "        if dist[i,b] > ratio*dist[i, a]:\n",
    "            good_matches.append([i,a])\n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test threading\n",
    "def _op(x, results, i):\n",
    "    results[i] = np.sum(x)\n",
    "\n",
    "if 'x_total' in locals():\n",
    "    del x_total\n",
    "#x_total = np.random.randint(1, 1000, size=(100_000_000))\n",
    "x_total = np.arange(150_000_000_0)\n",
    "#print(x_total.shape)\n",
    "#print(x_total[x_total.shape[0]//2:].shape)\n",
    "t = time.time()\n",
    "num_t = 12\n",
    "piece = x_total.shape[0]//num_t\n",
    "results = [None] * num_t\n",
    "ts = [*[threading.Thread(target=_op, args=(x_total[i*piece:(i+1)*piece],results, i)) for i in range(num_t-1)], threading.Thread(target=_op, args=(x_total[piece*(num_t-1):],results, num_t-1))]\n",
    "for thread in ts:\n",
    "    thread.start()\n",
    "for thread in ts:\n",
    "    thread.join()\n",
    "result_t = sum(results)\n",
    "t = time.time() - t\n",
    "print('Multithreading takes {:.2f} seconds\\tResult: {}'.format(t, result_t))\n",
    "t = time.time()\n",
    "result = [None]\n",
    "_op(x_total, result, 0)\n",
    "result_n = result[0]\n",
    "t = time.time() - t\n",
    "print('Normal takes {:.2f} seconds\\t\\tResult: {}'.format(t, result_n))\n",
    "if abs(result_t-result_n) > 0:\n",
    "    print('Something gone wrong')\n",
    "del x_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _thread_match(query, data, out_field, i):\n",
    "    out_field[i] = np.sum([np.square(data - q) for q in query], axis=2)\n",
    "\n",
    "def own_matching_threading(query, data, ratio=.75, num_threads=12):\n",
    "    piece = query.shape[0]//num_threads\n",
    "    results = [None] * num_threads\n",
    "    ts = [*[threading.Thread(target=_thread_match, args=(query[i*piece:(i+1)*piece,:],data,results, i)) for i in range(num_threads-1)], threading.Thread(target=_thread_match, args=(query[piece*(num_threads-1):,:],data,results, num_threads-1))]\n",
    "    for thread in ts:\n",
    "        thread.start()\n",
    "    for thread in ts:\n",
    "        thread.join()\n",
    "    #print(results[0].shape)\n",
    "    dist = np.concatenate(results)\n",
    "    #print(dist.shape)\n",
    "    \n",
    "    matches = np.argsort(dist)[:,:2]\n",
    "    good_matches = []\n",
    "    for i, (a, b) in enumerate(matches): ## i: query index | a,b: data indices\n",
    "        if dist[i,b] > ratio*dist[i, a]:\n",
    "            good_matches.append([i,a])\n",
    "    return good_matches\n",
    "\n",
    "#test = [None]\n",
    "#_thread_match(query_desc[:20,:], neighbor_desc, test, 0)\n",
    "#print(test[0].shape)\n",
    "print('Creating test arrays')\n",
    "m = 1000\n",
    "n = 500\n",
    "d = 128\n",
    "test_q = np.random.randint(0, 255, size=(m, d), dtype=np.uint8)\n",
    "test_d = np.random.randint(0, 255, size=(n, d), dtype=np.uint8)\n",
    "t = time.time()\n",
    "matches = own_matching_threading(test_q, test_d)\n",
    "t = time.time() - t\n",
    "print('Threading took {:.2f} seconds'.format(t))\n",
    "t = time.time()\n",
    "matches = own_matching(test_q, test_d)\n",
    "t = time.time() - t\n",
    "print('Normal took {:.2f} seconds'.format(t))\n",
    "t = time.time()\n",
    "matcher = cv2.BFMatcher.create(cv2.NORM_L2)\n",
    "matches = matcher.knnMatch(test_d, test_q, k=2)\n",
    "good = []\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "t = time.time() - t\n",
    "print('BF took {:.2f} seconds'.format(t))\n",
    "del matches, test_q, test_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_d = np.array([[1, 5],[-2,4], [3,7],[1,2]])\n",
    "test_vector_q = np.array([[2,1], [5,3],[1,1]])\n",
    "\n",
    "def LSH_matching(query, data, ratio=.75, k=2, buckets=5):\n",
    "    t = time.time()\n",
    "    print(query.shape)\n",
    "    print(data.shape)\n",
    "    engine = nearpy.Engine(data.shape[1], lshashes=[nearpy.hashes.RandomBinaryProjections('rbp', buckets)])\n",
    "    for i, v in enumerate(data):\n",
    "        engine.store_vector(v, '%d'%i)\n",
    "    print('Find neighbors')\n",
    "    nbrs = [engine.neighbours(d) for d in query]\n",
    "    print('Indices has shape ({},{})'.format(len(nbrs), len(nbrs[0])))\n",
    "    #print(nbrs[0])\n",
    "    print('minimal length of indices is {}'.format(min([len(i) for i in nbrs])))\n",
    "    #return None\n",
    "    print(nbrs[0][0][0].shape)\n",
    "    indices = np.stack([np.array([int(n[1]) for n in nbr])[:k] if len(nbr) > (k-1) else [-1]*k for nbr in nbrs ])\n",
    "    print('Indices has shape {}'.format(indices.shape))\n",
    "    \n",
    "    good_matches = []\n",
    "    dist = lambda i,j: np.sum(np.square(query[i] - data[j]))\n",
    "    for i, (a, b) in enumerate(indices): ## i: query index | a,b: data indices\n",
    "        if a > -1 and dist(i,b) > ratio*dist(i, a):\n",
    "            good_matches.append([i,a])\n",
    "    \n",
    "    \n",
    "    t = time.time() - t\n",
    "    print('LSH hashing takes {:.2f} seconds'.format(t))\n",
    "    return good_matches\n",
    "    \n",
    "matches = LSH_matching(query_desc, neighbor_desc)\n",
    "#print(own_matching(test_vector_q, test_vector_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m, n, d = 1_000, 1_000, 128\n",
    "    inp_x = np.random.randint(0,255,size=(m,d),dtype=np.uint8)\n",
    "    inp_y = np.random.randint(0,255,size=(n,d),dtype=np.uint8)\n",
    "    x = torch.as_tensor(inp_x, dtype=torch.float32).cuda()\n",
    "    y = torch.as_tensor(inp_y, dtype=torch.float32).cuda()\n",
    "    #x = torch.from_numpy(inp_x).cuda()\n",
    "    #y = torch.from_numpy(inp_y).cuda()\n",
    "    t = time.time()\n",
    "    x_y = torch.sum(torch.stack([(x-y_i)**2 for y_i in y]), (2,))\n",
    "    t = time.time() - t\n",
    "    x_y = x_y.cpu()\n",
    "    print('Normal took {:.2f} seconds'.format(t))\n",
    "    print(x_y.size())\n",
    "    t = time.time()\n",
    "    x_y = torch.stack([x.dist(y_i) for y_i in y])\n",
    "    t = time.time() - t\n",
    "    print('Dist took {:.2f} seconds'.format(t))\n",
    "del inp_x, inp_y, x, y, x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_desc.shape)\n",
    "print(neighbor_desc.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def own_matching_pytorch(query, data, ratio=.75, k=2, cuda=torch.cuda.is_available()):\n",
    "    t = time.time()\n",
    "    with torch.no_grad():\n",
    "        query_t = torch.as_tensor(query)\n",
    "        data_t = torch.as_tensor(data)\n",
    "        if cuda:\n",
    "            query_t = query_t.cuda()\n",
    "            data_t = data_t.cuda()\n",
    "        print('To cuda took {:.2f} seconds'.format(time.time() - t))\n",
    "        t = time.time()\n",
    "        dist = torch.sum(torch.stack([data_t.sub(query_t[i])**2 for i in range(query_t.size()[0])]), (2,))\n",
    "        #dist = torch.sum(subtract, (2,))\n",
    "        dist = dist.cpu().numpy()\n",
    "    print('Operation took {:.2f} seconds'.format(time.time() - t))\n",
    "    t = time.time()\n",
    "    matches = np.argsort(dist)[:,:2]\n",
    "    print('Sort took {:.2f} seconds'.format(time.time() - t))\n",
    "    t = time.time()\n",
    "    good_matches = []\n",
    "    for i, (a, b) in enumerate(matches): ## i: query index | a,b: data indices\n",
    "        if dist[i,b] > ratio*dist[i, a]:\n",
    "            good_matches.append([i,a])\n",
    "    print('Rest took {:.2f} seconds'.format(time.time()- t))\n",
    "    return good_matches\n",
    "    \n",
    "m = 1000\n",
    "n = 500\n",
    "d = 128\n",
    "test_q = np.random.rand(m, d)\n",
    "test_d = np.random.rand(n, d)\n",
    "\n",
    "torch.cuda.empty_cache()  \n",
    "t = time.time()\n",
    "matches = own_matching_pytorch(test_q, test_d)\n",
    "t = time.time() - t\n",
    "print('Cuda version took {:.2f} seconds'.format(t))\n",
    "del test_q, test_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_matching(desc1, desc2, ratio_thresh=.75, labels=None):\n",
    "    '''A fast matching method that matches multiple descriptors simultaneously.\n",
    "       Assumes that descriptors are normalized and can run on GPU if available.\n",
    "       Performs the landmark-aware ratio test if labels are provided.\n",
    "    '''\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    desc1, desc2 = torch.from_numpy(desc1).float(), torch.from_numpy(desc2).float()\n",
    "    if cuda:\n",
    "        desc1, desc2 = desc1.cuda(), desc2.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dist = 2*(1 - desc1 @ desc2.t())\n",
    "        dist_nn, ind = dist.topk(2, dim=-1, largest=False)\n",
    "        match_ok = (dist_nn[:, 0] <= (ratio_thresh**2)*dist_nn[:, 1])\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = torch.from_numpy(labels)\n",
    "            if cuda:\n",
    "                labels = labels.cuda()\n",
    "            labels_nn = labels[ind]\n",
    "            match_ok |= (labels_nn[:, 0] == labels_nn[:, 1])\n",
    "\n",
    "        if match_ok.any():\n",
    "            matches = torch.stack(\n",
    "                [torch.nonzero(match_ok)[:, 0], ind[match_ok][:, 0]], dim=-1)\n",
    "        else:\n",
    "            matches = ind.new_empty((0, 2))\n",
    "\n",
    "    return matches.cpu().numpy()\n",
    "\n",
    "t = time.time()\n",
    "fast_matching(query_desc, neighbor_desc)\n",
    "t = time.time() - t\n",
    "print('Fast matching takes {:.2f} seconds'.format(t))\n",
    "t = time.time()\n",
    "matcher = cv2.BFMatcher.create(cv2.NORM_L2)\n",
    "matches = matcher.knnMatch(query_desc, neighbor_desc, k=2)\n",
    "good = []\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "t = time.time() - t\n",
    "print('BF takes {:.2f} seconds'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __bf_matching__(x, y, ratio_thresh=0.75):\n",
    "    matcher = cv2.BFMatcher.create(cv2.NORM_L2)\n",
    "    matches = matcher.knnMatch(x, y, k=2)\n",
    "    good = []\n",
    "    for i,(m,n) in enumerate(matches):\n",
    "        if m.distance < ratio_thresh*n.distance:\n",
    "            good.append(m)\n",
    "    matches = np.array([[g.trainIdx, g.queryIdx] for g in good])\n",
    "    return matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __approx_np__(x,y, ratio_thresh=.5):\n",
    "    ##normalize\n",
    "    #t = time.time()\n",
    "    x = x.astype(np.float32)/np.linalg.norm(x.astype(np.float32),axis=-1, keepdims=True)\n",
    "    y = y.astype(np.float32)/np.linalg.norm(y.astype(np.float32),axis=-1, keepdims=True)\n",
    "    #t = time.time() - t\n",
    "    #print('Normalization takes {:.2f} s'.format(t))\n",
    "    #t = time.time()\n",
    "    d = 1.-np.matmul(y, x.T)\n",
    "    k = np.argpartition(d, 2, axis=1)[:,:2]\n",
    "    #intm = np.argsort(np.array([d[i, ap[i]] for i in range(ap.shape[0])]), axis=1)\n",
    "    #k = np.array([ap[i,a] for i, a in enumerate(intm)])\n",
    "    #t = time.time() - t\n",
    "    #print('Actual calculations take {:.2f} s'.format(t))\n",
    "    #t = time.time()\n",
    "    #matches = np.array([[i, k[i,0]] for i in range(d.shape[0]) if d[i, k[i, 0]] < (ratio_thresh**2)*d[i,k[i,1]]])\n",
    "    #t = time.time() - t\n",
    "    #print('Matching takes {:.2f} s'.format(t))\n",
    "    #print(d.shape)\n",
    "    #print(k.shape)\n",
    "    #print(matches.shape)\n",
    "    matches = []\n",
    "    for i, (m, n) in enumerate(k):\n",
    "        if d[i,m] < d[i,n]: \n",
    "            if d[i, m] < ratio_thresh*d[i, n]:\n",
    "                matches.append([i, m])\n",
    "        else:\n",
    "            if d[i, n] < ratio_thresh*d[i, m]:\n",
    "                matches.append([i, n])\n",
    "    return np.array(matches)\n",
    "    \n",
    "    \n",
    "t = time.time()\n",
    "#matches = __approx_np__(neighbor_desc, query_desc, ratio_thresh=.75)\n",
    "matches = __approx_np__(query_desc, neighbor_desc, ratio_thresh=.75)\n",
    "t = time.time() - t\n",
    "print('Took {:.2f} seconds'.format(t))\n",
    "print(matches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 0], [0, 1], [-1.0, -1.0]])\n",
    "y = np.array([[0.99, 0.1], [-1.0, 0.0], [0.0, -1.0], [0.5, 0.5], [.2, .8]])\n",
    "m = __approx_np__(x,y, ratio_thresh=.9)\n",
    "print(m.shape)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __to_unit_torch__(x, cuda):\n",
    "    if cuda:\n",
    "        x = torch.from_numpy(x).float().cuda()\n",
    "    else:\n",
    "        x = torch.from_numpy(x).float()\n",
    "    return (x.transpose(0, 1) / torch.norm(x, p=2, dim=1)).transpose(0,1)\n",
    "\n",
    "\n",
    "def __approx_torch__(x,y,ratio_thresh=.75):\n",
    "    cuda = torch.cuda.is_available()\n",
    "    x = __to_unit_torch__(x, cuda=cuda)\n",
    "    y = __to_unit_torch__(y, cuda=cuda)\n",
    "    with torch.no_grad():\n",
    "        d = 1. - torch.matmul(x, y.transpose(0,1))\n",
    "        values, indices = torch.topk(d, 2, dim=1, largest=False, sorted=True)\n",
    "        valid = values[:,0] < ratio_thresh**2*values[:,1]\n",
    "        if not torch.any(valid):\n",
    "            return np.array([])\n",
    "        indices_valid = indices[valid][:,0]\n",
    "        if cuda:\n",
    "            valid_indices = torch.arange(valid.size()[0]).cuda()[valid]\n",
    "        else:\n",
    "            valid_indices = torch.arange(valid.size()[0])[valid]\n",
    "        ret = torch.stack([torch.Tensor([valid_indices[i], indices_valid[i]]) for i in range(valid_indices.shape[0])])\n",
    "        return ret.cpu().numpy().astype(np.int64) if cuda else ret.numpy().astype(np.int64)\n",
    "    \"\"\"matches = []\n",
    "    for j, (v,i) in enumerate(zip(values, indices)):\n",
    "        if v[0] < ratio_thresh*v[1]:\n",
    "            matches.append([j, i[0]])\n",
    "    return np.array(matches)\n",
    "    \"\"\"\n",
    "    \n",
    "t = time.time()\n",
    "matches = __approx_torch__(neighbor_desc, query_desc, ratio_thresh=.75)\n",
    "t = time.time() - t\n",
    "print(matches.shape)\n",
    "print(matches.dtype)\n",
    "print('Took {:.2f} seconds'.format(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = np.flip(np.arange(12)).reshape(3, 4)\n",
    "#test = np.arange(12).reshape(3, 4)\n",
    "test = np.array([[0.0, 4.0, -1.0, 5.0], [5.0, 4.0, 2.0, 3.0], [4.0, 3.0, 2.0, 1.0]])\n",
    "print(test)\n",
    "print(test.shape)\n",
    "ap = np.argpartition(test, 2, axis=1)[:,:2]\n",
    "print(ap.shape)\n",
    "print(ap)\n",
    "test_ap = np.array([test[i, ap[i]] for i in range(ap.shape[0])])\n",
    "print(test_ap)\n",
    "ap2 = np.argsort(test_ap, axis=1)\n",
    "print(ap2)\n",
    "k = np.array([ap[i,a] for i, a in enumerate(ap2)])\n",
    "print(k.shape)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_type = 'torch_approx' # choices = ['Flann', 'BF', 'own']\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "print(query_desc.shape)\n",
    "print(neighbor_desc.shape)\n",
    "if matching_type == 'BF':\n",
    "    matches = __bf_matching__(neighbor_desc, query_desc)\n",
    "elif matching_type == 'approx':\n",
    "    matches = __approx_np__(neighbor_desc, query_desc)\n",
    "elif matching_type == 'torch_approx':\n",
    "    matches = __approx_torch__(query_desc, neighbor_desc)\n",
    "elif matching_type == 'own':\n",
    "    matches = own_matching(neighbor_desc, query_desc)\n",
    "elif matching_type == 'own_threading':\n",
    "    matches = own_matching_threading(neighbor_desc, query_desc)\n",
    "elif matching_type == 'pytorch':\n",
    "    matches = own_matching_pytorch(neighbor_desc, query_desc)\n",
    "elif matching_type == 'LSH':\n",
    "    matches = LSH_matching(neighbor_desc, query_desc)\n",
    "elif matching_type == 'fast':\n",
    "    q = query_desc.astype(np.float32)   / np.linalg.norm(query_desc.astype(np.float32),    axis=-1, keepdims=True)\n",
    "    n = neighbor_desc.astype(np.float32)/ np.linalg.norm(neighbor_desc.astype(np.float32), axis=-1, keepdims=True)\n",
    "    matches = fast_matching(n, q, ratio_thresh=.75)\n",
    "else:\n",
    "    raise NotImplementedError('Not Implemented')\n",
    "print(matches.shape)\n",
    "matches = [cv2.DMatch(_queryIdx=m[1], _trainIdx=m[0], _imgIdx=0, _distance=1.0) for m in matches]\n",
    "t = time.time() - t\n",
    "print('Matching took {:.1f} seconds\\nFound {} matches'.format(t, len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.drawMatchesKnn expects list of lists as matches.\n",
    "#draw_params = dict(matchColor = (0,255,0),\n",
    "#                   singlePointColor = (255,0,0),\n",
    "#                   matchesMask = matchesMask,\n",
    "#                   flags = 2)\n",
    "img3 = np.empty((max(query_img.shape[0], neighbor_img.shape[0]), query_img.shape[1] + neighbor_img.shape[1], 3), dtype=np.uint8)\n",
    "cv2.drawMatches(neighbor_img_original,neighbor_kpts,np.array(query_imgs_high_res[query_id]),query_kpts,matches,outImg=img3,matchColor=None, singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img3)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for g in good:\n",
    "#    print(g.queryIdx)\n",
    "kpts_good = [query_kpts[i] for i in [g.trainIdx for g in good]]\n",
    "dataset_sift_img_good=cv2.drawKeypoints(np.array(query_imgs_high_res[query_id]),kpts_good, None)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(dataset_sift_img_good)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RangeDict(dict):\n",
    "    def __getitem__(self, item):\n",
    "        if type(item) != range: # or xrange in Python 2\n",
    "            for key in self:\n",
    "                if item in key:\n",
    "                    return self[key]\n",
    "        else:\n",
    "            return super().__getitem__(item)\n",
    "\n",
    "test_dict = RangeDict({range(1, 20) : 'Low', range(20, 35) : 'Medium'})\n",
    "test_dict[range(35, 50)] = 'High'\n",
    "print(test_dict[10])\n",
    "print(test_dict[25])\n",
    "print(test_dict[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = __bf_matching__(query_desc, neighbor_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##settings\n",
    "ratio_thresh = 0.75# 0.75\n",
    "matching_type = 'torch_approx'\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "matched_kpts_cv = []\n",
    "matched_pts = []\n",
    "print('Start')\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        db_id = img # get_img_id_dataset(database_cursor, img)\n",
    "        img_name = images[db_id].name\n",
    "        valid = images[db_id].point3D_ids > 0 \n",
    "        data_kpts, data_desc = get_kpts_desc(database_cursor, db_id)\n",
    "        data_kpts = kpts_to_cv(data_kpts[valid[:data_kpts.shape[0]]] - 0.5)\n",
    "        pt_ids = images[db_id].point3D_ids[valid]\n",
    "        data_desc = data_desc[valid[:data_desc.shape[0]]]\n",
    "\n",
    "        if matching_type == 'BF':\n",
    "            matches = __bf_matching__(query_desc, data_desc)\n",
    "        elif matching_type == 'approx':\n",
    "            matches = __approx_np__(query_desc, data_desc)\n",
    "        elif matching_type == 'torch_approx':\n",
    "            matches = __approx_torch__(data_desc, query_desc)\n",
    "        elif matching_type == 'own':\n",
    "            matches = own_matching(query_desc, data_desc)\n",
    "        elif matching_type == 'own_threading':\n",
    "            matches = own_matching_threading(query_desc, data_desc, num_threads=8)\n",
    "        elif matching_type == 'pytorch':\n",
    "            matches = own_matching_pytorch(query_desc, data_desc)\n",
    "        elif matching_type == 'LSH':\n",
    "            matches = LSH_matching(query_desc, data_desc, buckets = 10)\n",
    "        elif matching_type == 'fast':\n",
    "            q = query_desc.astype(np.float32)/ np.linalg.norm(query_desc.astype(np.float32),    axis=-1, keepdims=True)\n",
    "            n = data_desc.astype(np.float32)/ np.linalg.norm(data_desc.astype(np.float32), axis=-1, keepdims=True)\n",
    "            matches = fast_matching(q, n, ratio_thresh=ratio_thresh)\n",
    "            \n",
    "        matched_kpts_cv += [query_kpts[m[1]] for m in matches]\n",
    "        matched_pts += [pt_ids[m[0]] for m in matches]\n",
    "        \n",
    "\n",
    "t = time.time() - t\n",
    "print('Total matching time: {:.2f} seconds'.format(t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##settings\n",
    "ratio_thresh = 0.75# 0.75\n",
    "\n",
    "\n",
    "def _bf_thread(img, data_desc, matched_kpts_cv, matched_pts):\n",
    "    valid = images[img].point3D_ids > 0 \n",
    "    pt_ids = images[img].point3D_ids[valid]\n",
    "    data_desc = data_desc[valid[:data_desc.shape[0]]]\n",
    "    matches = matcher.knnMatch(data_desc,query_desc,k=2)\n",
    "    good = []\n",
    "    for i,(m,n) in enumerate(matches):\n",
    "        if m.distance < ratio_thresh*n.distance:\n",
    "            good.append(m)\n",
    "    matched_kpts_cv += [query_kpts[i] for i in [g.trainIdx for g in good]]\n",
    "    matched_pts += [pt_ids[i] for i in [g.queryIdx for g in good]]\n",
    "\n",
    "t = time.time()\n",
    "matched_kpts_cv = []\n",
    "matched_pts = []\n",
    "matcher = cv2.BFMatcher.create(cv2.NORM_L2)\n",
    "print('Start')\n",
    "ts = []\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        img_name = images[img].name\n",
    "        valid = images[img].point3D_ids > 0 \n",
    "        _, data_desc = get_kpts_desc(database_cursor, img)\n",
    "        ts.append(threading.Thread(target=_bf_thread, args=(img, data_desc, matched_kpts_cv, matched_pts)))\n",
    "for thread in ts:\n",
    "    thread.start()\n",
    "for thread in ts:\n",
    "    thread.join()\n",
    "\n",
    "t = time.time() - t\n",
    "print('Total matching time: {:.2f} seconds'.format(t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##settings \n",
    "ratio_thresh = 0.75# 0.75\n",
    "matching_type = 'BF'\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "matched_kpts_cv = []\n",
    "matched_pts = []\n",
    "if matching_type is 'BF':\n",
    "    matcher = cv2.BFMatcher.create(cv2.NORM_L2)\n",
    "elif matching_type is 'Flann':\n",
    "    index_params = dict(algorithm=0, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    query_desc = query_desc.astype(np.float32)\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "#else:\n",
    "#    raise NotImplementedError('Not Implemented')\n",
    "print('Start')\n",
    "pt_id_list = []\n",
    "data_descs = []\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        db_id = img # get_img_id_dataset(database_cursor, img)\n",
    "        img_name = images[db_id].name\n",
    "        valid = images[db_id].point3D_ids > 0 \n",
    "        data_kpts, data_desc = get_kpts_desc(database_cursor, db_id)\n",
    "        data_kpts = kpts_to_cv(data_kpts[valid[:data_kpts.shape[0]]] - 0.5)\n",
    "        pt_ids = images[db_id].point3D_ids[valid]\n",
    "        pt_id_list.append(pt_ids)\n",
    "        data_desc = data_desc[valid[:data_desc.shape[0]]]\n",
    "        #print('Point Ids: {}\\t Desc shape: {}'.format(len(pt_ids), data_desc.shape[0]))\n",
    "        data_descs.append(data_desc)\n",
    "\n",
    "data_descs = np.concatenate(data_descs)\n",
    "pt_id_list = np.concatenate(pt_id_list)\n",
    "if matching_type in ['BF', 'Flann']:\n",
    "    matches = matcher.knnMatch(data_descs,query_desc,k=2)\n",
    "    good = []\n",
    "    for i,(m,n) in enumerate(matches):\n",
    "        if m.distance < ratio_thresh*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    matched_kpts_cv = [query_kpts[i] for i in [g.trainIdx for g in good]]\n",
    "    matched_pts = [pt_id_list[i] for i in [g.queryIdx for g in good]]\n",
    "elif matching_type is 'own':\n",
    "    matches = own_matching(query_desc, data_descs)\n",
    "elif matching_type is 'own_threading':\n",
    "    matches = own_matching_threading(query_desc, data_descs, num_threads=24)\n",
    "elif matching_type is 'pytorch':\n",
    "    matches = own_matching_pytorch(query_desc, data_descs)\n",
    "elif matching_type is 'LSH':\n",
    "    matches = LSH_matching(query_desc, data_descs, buckets = 10)\n",
    "\n",
    "t = time.time() - t\n",
    "print('Total matching time: %d seconds'%t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t = time.time()\n",
    "matched_kpts_cv_x = []\n",
    "matched_pts_x = []\n",
    "matcher = cv2.BFMatcher.create(cv2.NORM_L2)\n",
    "query_desc = query_desc.astype(np.uint8)\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        db_id = img # get_img_id_dataset(database_cursor, img)\n",
    "        img_name = images[db_id].name\n",
    "        valid = images[db_id].point3D_ids > 0 \n",
    "        #if args.local_method is 'Colmap':\n",
    "        data_kpts, data_desc = get_kpts_desc(database_cursor, db_id)\n",
    "        data_kpts = kpts_to_cv(data_kpts[valid[:data_kpts.shape[0]]] - 0.5)\n",
    "        #else:\n",
    "        #    raise NotImplementedError('Local method not implemented in matching')\n",
    "        pt_ids = images[db_id].point3D_ids[valid]\n",
    "        data_desc = data_desc[valid[:data_desc.shape[0]]]\n",
    "\n",
    "        matches_x = matcher.knnMatch(data_desc, query_desc, k=2)\n",
    "        good_x = []\n",
    "        for i,(m,n) in enumerate(matches_x):\n",
    "            if m.distance < ratio_thresh*n.distance:\n",
    "                good_x.append(m)\n",
    "        matched_kpts_cv_x += [query_kpts[i] for i in [g.trainIdx for g in good_x]]\n",
    "        matched_pts_x += [pt_ids[i] for i in [g.queryIdx for g in good_x]]\n",
    "        #print('.',end='')\n",
    "t = time.time() - t\n",
    "print('{:1f} seconds\\t {} matches'.format(t, len(matched_pts_x)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(matched_pts))\n",
    "print(matched_pts[0])\n",
    "print(matched_pts_x[0])\n",
    "print([i for i, (x,y) in enumerate(zip(matched_pts_x, matched_pts)) if (x-y) != 0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "points_to_match = []\n",
    "key_point = []\n",
    "breaker = 0\n",
    "break_at = 5\n",
    "ratio_thresh = 0.75\n",
    "t = time.time()\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        \"\"\"\n",
    "        kpts = []\n",
    "        point_ids = measurements[img]['point_id']\n",
    "        for i, kpt in enumerate(measurements[img]['kpts']):\n",
    "            kpts.append(cv2.KeyPoint(kpt[0], kpt[1], _size=med_kpt_size))\n",
    "        if len(kpts) <= 0:\n",
    "            continue\n",
    "        _, desc = sift.compute(np.array(dataset[img][0]), kpts)\n",
    "        \"\"\"\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "        matches = matcher.knnMatch(desc, query_desc, k=2)\n",
    "        good = []\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < ratio_thresh*n.distance:\n",
    "                #matchesMask[i]=[1,0]\n",
    "                good.append(m)\n",
    "        kpts_good = [query_kpts[i] for i in [g.trainIdx for g in good]]\n",
    "        key_point += kpts_good\n",
    "        points_to_match += [point_ids[i] for i in [g.queryIdx for g in good]]\n",
    "        breaker += 1\n",
    "        if breaker < break_at:\n",
    "            fig = plt.figure(figsize=(10, 15))\n",
    "            a = fig.add_subplot(1, 2, 1)\n",
    "            plt.imshow(cv2.drawKeypoints(query_img,kpts_good, None))\n",
    "            data_img = np.array(dataset[img][0])\n",
    "            img3 = np.empty((max(query_sift_img.shape[0], data_img.shape[0]), query_sift_img.shape[1] + data_img.shape[1], 3), dtype=np.uint8)\n",
    "            cv2.drawMatches(data_img,kpts,query_img,query_kpts,good,outImg=img3,matchColor=None, singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "            a = fig.add_subplot(1, 2, 2)\n",
    "            plt.imshow(img3)\n",
    "            plt.show()\n",
    "t = time.time() - t\n",
    "print('Total matching time: %d seconds'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_pts_xyz = np.stack([points3d[i].xyz for i in matched_pts])\n",
    "print(matched_pts_xyz.mean(axis=0))\n",
    "print(matched_pts_xyz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_keypoints = np.vstack([np.array([x.pt[0], x.pt[1]]) for x in matched_kpts_cv])\n",
    "print(matched_keypoints.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = path_to_queries[query_id].replace('data/AachenDayNight/images_upright/', '')\n",
    "cm = camera_matrices[query_path]\n",
    "camera_matrix = cm['cameraMatrix']\n",
    "distortion_coeff = cm['rad_dist']\n",
    "print(camera_matrix)\n",
    "print(distortion_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "dist_vec = np.array([distortion_coeff, 0, 0, 0])\n",
    "success, R_vec, translation, inliers = cv2.solvePnPRansac(\n",
    "        matched_pts_xyz, matched_keypoints, camera_matrix, dist_vec,\n",
    "        iterationsCount=n_iter, reprojectionError=10.,\n",
    "        flags=cv2.SOLVEPNP_P3P)\n",
    "t = time.time() - t\n",
    "print('PnP RANSAC took %d seconds (%.2f/iteration)'%(t, float(t)/float(n_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    print('Successful matching')\n",
    "    print(inliers.shape)\n",
    "else:\n",
    "    print('Not succesful')\n",
    "print(R_vec)\n",
    "print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_pts_xyz[inliers].shape)\n",
    "print(matched_keypoints[inliers].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_inliers = 10\n",
    "\n",
    "if success:\n",
    "    inliers = inliers[:, 0] if len(inliers.shape) > 1 else inliers\n",
    "    num_inliers = len(inliers)\n",
    "    inlier_ratio = len(inliers) / len(matched_keypoints)\n",
    "    success &= num_inliers >= min_inliers\n",
    "\n",
    "    ret, R_vec, t = cv2.solvePnP(\n",
    "                matched_pts_xyz[inliers], matched_keypoints[inliers], camera_matrix,\n",
    "                dist_vec, rvec=R_vec, tvec=translation, useExtrinsicGuess=True,\n",
    "                flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    assert ret\n",
    "\n",
    "    query_T_w = np.eye(4)\n",
    "    query_T_w[:3, :3] = cv2.Rodrigues(R_vec)[0]\n",
    "    query_T_w[:3, 3] = t[:, 0]\n",
    "    w_T_query = np.linalg.inv(query_T_w)\n",
    "\n",
    "    #ret = LocResult(success, num_inliers, inlier_ratio, w_T_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_T_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(w_T_query)\n",
    "qvec_nvm = list(Quaternion(matrix=query_T_w)) # rotmat2qvec(w_T_query[:3,:3])\n",
    "pos = -txq.rotate_vector(w_T_query[:3,3], qvec_nvm)\n",
    "print('Calculated position: {}\\nOrientation: {}'.format(w_T_query[:3, 3], qvec_nvm))\n",
    "print('Transformed position: {}'.format(pos))\n",
    "if query_id > 3:\n",
    "    #pose_stats_filename = os.path.join('data/AachenDayNight/', 'pose_stats.txt')\n",
    "    #mean_t, std_t = np.loadtxt(pose_stats_filename)\n",
    "    #position = dataset[dataset_queries[query_id-4]][1][:3]\n",
    "    #position = position*std_t + mean_t\n",
    "    position = colmap_image_to_pose(images[get_img_id(database_cursor, fake_query_path)])[:3,3]\n",
    "    rotation = images[get_img_id(database_cursor, fake_query_path)].qvec\n",
    "    \n",
    "    #query_T_w = np.linalg.inv(result.T)\n",
    "    #pos_nvm = query_T_w[:3, 3].tolist()\n",
    "    \n",
    "    error_rot = quaternion_angular_error(rotation, qvec_nvm)\n",
    "    error = np.linalg.norm(position-w_T_query[:3,3])\n",
    "    error_str = '%.1f m'%error if error > 1e-1 else '%.1f cm'%(100.0*error)\n",
    "    print('Groundtruth: \\t%s \\nError transl.: \\t%s\\nAngular error: \\t%.2f°'%(str(position), error_str, error_rot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_id == 2:\n",
    "    qvec_mapnet = np.array([0.14751838147640228, 0.7848054766654968, -0.09522610157728195, -0.5943489670753479])\n",
    "    pos_mapnet = np.array([-379.47140824838095, 212.66002640437915, 609.4124687897538])\n",
    "    print('Difference position: {:.2f}m'.format(np.linalg.norm(pos_mapnet - pos)))\n",
    "    print('Difference orientation: {:.2f}°'.format(quaternion_angular_error(qvec_mapnet, qvec_nvm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_T_query[:3,3])\n",
    "print(-txq.rotate_vector(w_T_query[:3,3], qvec_nvm))\n",
    "print(-txq.rotate_vector(pos_mapnet, qvec_mapnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_poses(corrupt_file, new_file_name):\n",
    "    new_file = open(new_file_name, 'w')\n",
    "    with open(corrupt_file, 'r') as f:\n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "        for l in lines:\n",
    "            splt = l.split(' ')\n",
    "            pos = [float(s) for s in splt[-3:]]\n",
    "            w_T = np.zeros((4,4))\n",
    "            w_T[3,3] = 1.0\n",
    "            w_T[:3,3] = pos\n",
    "            w_T[:3,:3] = qvec2rotmat([float(s) for s in splt[1:5]])\n",
    "            T_w = np.linalg.inv(w_T.T)\n",
    "            quat = list(Quaternion(matrix=T_w))\n",
    "            pos = -txq.rotate_vector(np.array(pos), np.array(quat))\n",
    "            img_string = '{} {} {} {} {} {} {} {}\\n'.format(splt[0], *list(quat), *pos)\n",
    "            new_file.write(img_string)\n",
    "    new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_poses('aachen_eval_slurm_5.txt', 'submissions/Aachen_eval_conet_corrected.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop notebook run all\n",
    "1.0/0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now all clusters\n",
    "t = time.time()\n",
    "kpts_des = {}\n",
    "place_lms = []\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        kpts = []\n",
    "        point_ids = measurements[img]['point_id']\n",
    "        place_lms += point_ids\n",
    "        for i, kpt in enumerate(measurements[img]['kpts']):\n",
    "            kpts.append(cv2.KeyPoint(kpt[0], kpt[1], _size=med_kpt_size))\n",
    "        if len(kpts) <= 0:\n",
    "            continue\n",
    "        kpts_des[img] = sift.compute(np.array(dataset[img][0]), kpts)\n",
    "place_lms = np.array(place_lms)\n",
    "t = time.time() - t\n",
    "print('Took %d seconds'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = list(cluster_query[0])[12]\n",
    "plt.imshow(cv2.drawKeypoints(np.array(dataset[random_id][0]),kpts_des[random_id][0], None))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kpts_des))\n",
    "print(sum([len(c) for c in cluster_query]))\n",
    "print(place_lms.shape)\n",
    "print(kpts_des[random_id][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://github.com/ethz-asl/hfnet\n",
    "\"\"\"\n",
    "\n",
    "def matches_cv2np(matches_cv):\n",
    "    matches_np = np.int32([[m.queryIdx, m.trainIdx] for m in matches_cv])\n",
    "    distances = np.float32([m.distance for m in matches_cv])\n",
    "    return matches_np.reshape(-1, 2), distances\n",
    "\n",
    "query_des = query_des.astype(np.float32, copy=False)\n",
    "dataset_des = np.concatenate([kpts_des[x][1] for x in kpts_des]).astype(np.float32, copy=False)\n",
    "#img_match_ids = [x for i in range(len(kpts_des[x][1])) for x in kpts_des]\n",
    "#print(len(img_match_ids))\n",
    "\n",
    "\n",
    "print(type(query_des))\n",
    "print(type(dataset_des))\n",
    "print(query_des.shape)\n",
    "print(dataset_des.shape)\n",
    "\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "m = matcher.knnMatch(dataset_des, query_des, k=2)\n",
    "\n",
    "matches1, matches2 = list(zip(*m))\n",
    "(matches1, dist1) = matches_cv2np(matches1)\n",
    "(matches2, dist2) = matches_cv2np(matches2)\n",
    "print(matches1.shape)\n",
    "print(matches2.shape)\n",
    "print('Done matching: %d matches'%len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_thresh = 0.70\n",
    "#print(matches1[:5])\n",
    "#print(matches2[:5])\n",
    "#print(matches1[103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(place_lms.shape)\n",
    "## Index testing cannot work as we do not know which points are in query (problem would be already solved)\n",
    "#good = (place_lms[matches1[:, 1]] == place_lms[matches2[:, 1]])\n",
    "good = (dist1/dist2 < ratio_thresh)\n",
    "matches_good = matches1[good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(good[:10])\n",
    "print(matches_good[0])\n",
    "#print(len([g for g in good if not g]))\n",
    "#print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "img_shown = 0\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        kpt = kpts_des[img][0]\n",
    "        n = len(kpt)\n",
    "        cnt += n\n",
    "        print('%d:%d'%(cnt-n,cnt))\n",
    "        if not np.any(good[cnt-n:cnt]):\n",
    "            continue\n",
    "        \n",
    "        img_shown += 1\n",
    "        if img_shown > 5:\n",
    "            break\n",
    "        #matches_img = [i for x in m[cnt-n:cnt] for i in x if x[0].distance < ratio_thresh*x[1].distance]\n",
    "        matches_img = [cv2.DMatch()]\n",
    "        data_img = np.array(dataset[img][0])\n",
    "        img3 = np.empty((max(query_sift_img.shape[0], data_img.shape[0]), query_sift_img.shape[1] + data_img.shape[1], 3), dtype=np.uint8)\n",
    "        cv2.drawMatches(data_img,kpt,query_img,query_kpts,matches_img,outImg=img3,matchColor=None, singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "        plt.imshow(img3)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_points = np.stack([points[place_lms[i]] for i in matches[:, 0]])\n",
    "matched_kpts = np.vstack([np.array(query_kpts[i].pt) for i in matches[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_points.shape)\n",
    "print(matched_kpts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 300\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(matched_points[:,0], matched_points[:,2], matched_points[:,1], s = 1.5, alpha = 0.5)\n",
    "#for cp in cluster_points:\n",
    "#    ax.scatter3D(cp[:,0], cp[:,2], cp[:,1], s = 0.5, alpha = 0.25)\n",
    "#ax.scatter3D(translation[0], translation[2], translation[1], s=25, alpha=1.0)\n",
    "median = np.median(points, axis=0)\n",
    "print('Median is %s'%median)\n",
    "#ax.set_xlim3d(median[0]-thresh,median[0]+thresh)#min(sift_points[:,0]),min(sift_points[:,0])+max_dist)\n",
    "#ax.set_ylim3d(median[2]-thresh,median[2]+thresh)#min(sift_points[:,1]),min(sift_points[:,1])+max_dist)\n",
    "#ax.set_zlim3d(median[1]-50, median[1]+50)#min(sift_points[:,2]),min(sift_points[:,2])+max_dist)\n",
    "ax.view_init(elev=35., azim=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfnet-pytorch",
   "language": "python",
   "name": "hfnet-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
