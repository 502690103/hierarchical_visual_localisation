{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Model pipeline\n",
    "1. Find images with similar global descriptors\n",
    "2. Cluster by covisiblity\n",
    "3. Find local descriptors\n",
    "4. Match to SfM model\n",
    "5. Calculate pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports, Loading data\n",
    "Loading data into memory. This may take some minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from dataset_loaders import aachen\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "from dataset_loaders.utils import load_image\n",
    "import models.netvlad_vd16_pitts30k_conv5_3_max_dag as netvlad\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find for each image in dataset all other images that share common points\n",
    "\n",
    "Parameters:\n",
    "path: path to .nvm file\n",
    "threshold: Number of points that images at least need to share\n",
    "\n",
    "Returns:\n",
    "img_cluster: Dictionary {id: set of all images that share points with Image}\n",
    "points_of_img: Dictionary maps camera ids to point ids\n",
    "points: Position of points\n",
    "\"\"\"\n",
    "def read_colmap_file(path, threshold=2):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        num_points = int(lines[2].strip())\n",
    "        sift_lines = lines[num_points+4:]\n",
    "        #lines = lines[:num_points+3]\n",
    "        #lines[3:] = [x.strip().split(' ') for x in lines[3:]]\n",
    "        num_sifts = int(sift_lines[0])\n",
    "        sift_lines = [x.strip().split(' ') for x in sift_lines[1:]]\n",
    "        print('Read 3d model')\n",
    "        #points_per_img = {i:[] for i in range(num_points)}\n",
    "        \"\"\"\n",
    "        <Camera> = <File name> <focal length> <quaternion WXYZ> <camera center> <radial distortion> 0\n",
    "        <Point>  = <XYZ> <RGB> <number of measurements> <List of Measurements>\n",
    "        <Measurement> = <Image index> <Feature Index> <xy>\n",
    "        \"\"\"\n",
    "        img_cluster = {}     # camera_id : set of other cameras that share points\n",
    "        points = []          # 3d positions of points\n",
    "        points_rgb = []      # rgb values of points\n",
    "        measurements = {}    # camera_id : list of measurements(point_index, feat_id, xy)\n",
    "        points_of_img = {}   # camera_id : set of associated points\n",
    "        \n",
    "        ## Iterate over 3d points\n",
    "        for i in range(num_sifts):\n",
    "            if i % 500000 == 0:\n",
    "                print('%d/%d'%(i,num_sifts))\n",
    "            ids = []\n",
    "            line = sift_lines[i]\n",
    "            ## extract point info\n",
    "            xyz = np.array([float(x) for x in line[:3]], dtype=np.float32)\n",
    "            rgb = np.array([int(x) for x in line[3:6]], dtype=np.int)\n",
    "            points.append(xyz)\n",
    "            points_rgb.append(rgb)\n",
    "            ## only use measurements above threshold\n",
    "            num_imgs = int(line[6])\n",
    "            if num_imgs < threshold:\n",
    "                continue\n",
    "            ## iterate over measurements\n",
    "            for j in range(num_imgs):\n",
    "                camera_idx = int(line[7+j*4])\n",
    "                feat_idx = int(line[8+j*4])\n",
    "                xy = np.array([float(line[9+j*4]), float(line[10+j*4])], dtype=np.float32)\n",
    "                if camera_idx in measurements:\n",
    "                    measurements[camera_idx]['point_id'].append(i)\n",
    "                    measurements[camera_idx]['kpts'].append([xy])                    \n",
    "                else:\n",
    "                    measurements[camera_idx] = {'point_id' : [i], 'kpts': [xy]}\n",
    "                ids.append(camera_idx)\n",
    "                if camera_idx in points_of_img:\n",
    "                    points_of_img[camera_idx].add(i)\n",
    "                else:\n",
    "                    points_of_img[camera_idx] = {i}\n",
    "            for j in ids:\n",
    "                if j not in img_cluster:\n",
    "                    img_cluster[j] = set(ids)\n",
    "                else:\n",
    "                    img_cluster[j] |= set(ids)\n",
    "\n",
    "        points = np.vstack(points)\n",
    "        points_rgb = np.vstack(points_rgb)\n",
    "        for cam in measurements:\n",
    "            measurements[cam]['kpts'] = np.vstack(measurements[cam]['kpts'])\n",
    "        print('Done loading %d cameras and %d 3d points'%(len(img_cluster), points.shape[0]))\n",
    "        return img_cluster, points_of_img, points, points_rgb, measurements\n",
    "\n",
    "db_path = 'data/AachenDayNight/aachen_cvpr2018_db.nvm'\n",
    "threshold = 3 # How many points need to be at least shared between images\n",
    "img_cluster, points_of_img, points, points_rgb, measurements = read_colmap_file(db_path, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image sizes\n",
    "print('Loading all original image sizes (This may take some time)')\n",
    "img_sizes = []\n",
    "real_size_dataset = aachen.AachenDayNight('data/AachenDayNight', True, train_split=-1,seed=0,input_types='img', output_types=[], real=True, verbose=False)\n",
    "for i, (d, t) in enumerate(real_size_dataset):\n",
    "    if i % 1000 == 0:\n",
    "        print('%d/%d'%(i, len(real_size_dataset)))\n",
    "    img_sizes.append(np.array(d.size))\n",
    "    #if i not in measurements:\n",
    "    #    measurements[i] = {'point_id' : [], 'kpts': np.array([])}\n",
    "img_sizes = np.vstack(img_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera matrix\n",
    "camera_matrices = {}\n",
    "query_intrinsics_files = ['data/AachenDayNight/queries/day_time_queries_with_intrinsics.txt',\n",
    "                         'data/AachenDayNight/queries/night_time_queries_with_intrinsics.txt',\n",
    "                         'data/AachenDayNight/database_intrinsics.txt']\n",
    "for file_path in query_intrinsics_files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "        for line in lines:\n",
    "            # Format: `image_name SIMPLE_RADIAL w h f cx cy r`\n",
    "            line = line.split(' ')\n",
    "            img_path = line[0]\n",
    "            f = float(line[4])\n",
    "            cx = float(line[5])\n",
    "            cy = float(line[6])\n",
    "            rad_dist = float(line[7])\n",
    "            A = np.array([[f, 0, cx],[0, f, cy], [0, 0, 1]])\n",
    "            camera_matrices[img_path] = {'cameraMatrix': A, 'rad_dist':rad_dist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 256\n",
    "n_images = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2 day time queries\n",
    "2 night time queries\n",
    "2 dataset queries\n",
    "\"\"\"\n",
    "dataset_queries = [1, 500, 250, 2500, 4000]\n",
    "path_to_queries = [\n",
    "                   'data/AachenDayNight/images_upright/query/day/nexus5x/IMG_20161227_162905.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/day/nexus5x/IMG_20161227_160713.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/night/nexus5x/IMG_20161227_172616.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/night/nexus5x/IMG_20161227_191152.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/db/1.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/db/500.jpg'\n",
    "                  ] + ['data/AachenDayNight/images_upright/db/'+str(i)+'.jpg' for i in dataset_queries]\n",
    "n_queries = len(path_to_queries)\n",
    "transform = transforms.Compose([transforms.Resize(resolution), transforms.CenterCrop(resolution)])\n",
    "query_imgs_high_res = [load_image(path) for path in path_to_queries]\n",
    "query_imgs_low_res = [transform(img) for img in query_imgs_high_res]\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "plt.title('Queries')\n",
    "plt.axis('off')\n",
    "for i, img in enumerate(query_imgs_high_res):\n",
    "    a = fig.add_subplot(1, len(query_imgs_high_res), i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find similar images (global descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = netvlad.vd16_pitts30k_conv5_3_max_dag(weights_path='data/teacher_models/netvlad_pytorch/vd16_pitts30k_conv5_3_max_dag.pth')\n",
    "model.eval()\n",
    "query_global_desc = [model(transforms.ToTensor()(img).unsqueeze(0)).detach().cpu().squeeze(0).numpy() for img in query_imgs_low_res]\n",
    "query_global_desc = np.vstack(query_global_desc)\n",
    "print(query_global_desc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global descriptors for dataset are precalculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Find nearest neighbors for queries (This may take some time)')\n",
    "file = h5py.File(\"data/full_dataset_\"+str(resolution)+\".hdf5\", \"r\")\n",
    "verification = file['results']\n",
    "len_feat_vec = int(verification[0][0])\n",
    "print('%d data points with %d sized feature vectors'%(verification.shape[0], len_feat_vec))\n",
    "nbrs = NearestNeighbors(n_neighbors=n_images).fit(verification[:,1:len_feat_vec+1])\n",
    "print('Fitted')\n",
    "#distances, indices = nbrs.kneighbors(verification[:n_samples,1:verification[0].shape[0]])\n",
    "distances, indices = nbrs.kneighbors(query_global_desc)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aachen.AachenDayNight('data/AachenDayNight', True, train_split=-1,seed=0,#transform=transform,\n",
    "                                input_types='img', #output_types=[],\n",
    "                                real=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, query_img in enumerate(query_imgs_high_res):\n",
    "    imgs = [query_img] \n",
    "    imgs = imgs + [dataset[j][0] for j in indices[i]]\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    #plt.title('Neighbors')\n",
    "    for j, img in enumerate(imgs):\n",
    "        a = fig.add_subplot(n_queries, n_images+1, i*(n_images+1)+j+1)\n",
    "        plt.imshow(img)\n",
    "        if j > 0:\n",
    "            plt.title('%.0f'%distances[i][j-1])\n",
    "        else:\n",
    "            plt.title('Query')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covisibility clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_query = [img_cluster[indices[query_id][0]]]\n",
    "cluster_orig_ids = [indices[query_id][0]]\n",
    "for i, ind in enumerate(indices[query_id]):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    point_set = img_cluster[ind]\n",
    "    print('Match neighbor %d'%i)\n",
    "    disjoint = False\n",
    "    for j, c in enumerate(cluster_query):\n",
    "        if ind in c:\n",
    "            print('  - Can be matched to cluster')\n",
    "            #print(point_set - (point_set-cluster))\n",
    "            cluster_query[j] |= point_set\n",
    "            disjoint = True\n",
    "            break\n",
    "    if not disjoint:\n",
    "        print('  - New cluster created')\n",
    "        cluster_orig_ids.append(ind)\n",
    "        cluster_query.append(point_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs_per_cluster = 6\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "plt.title('Covisibility clustering')\n",
    "plt.axis('off')\n",
    "for i, cluster in enumerate(cluster_query):\n",
    "    imgs = list(cluster_query[i])[:num_imgs_per_cluster]\n",
    "    imgs = [dataset[cluster_orig_ids[i]][0]]+[dataset[j][0] for j in imgs]\n",
    "    for j, img in enumerate(imgs):\n",
    "        a = fig.add_subplot(len(cluster_query), len(imgs), i*len(imgs)+j+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        if j == 0:\n",
    "            plt.title('Clustered by')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_points = []\n",
    "for i, c in enumerate(cluster_query):\n",
    "    imgs_cluster = list(c)\n",
    "    points_cluster = set()\n",
    "    for ind in imgs_cluster:\n",
    "        points_cluster |= points_of_img[ind]\n",
    "    points_cluster = list(points_cluster)\n",
    "\n",
    "    mask = np.ones(points.shape[0],dtype=bool) #np.ones_like(a,dtype=bool)\n",
    "    mask[points_cluster] = False\n",
    "\n",
    "    cluster_points.append(points[~mask])\n",
    "    print('%d\\tpoints in cluster'%cluster_points[i].shape[0])\n",
    "#other_points = points[mask]\n",
    "#print('%d other points'%other_points.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 300\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.scatter3D(other_points[:,0], other_points[:,2], other_points[:,1], s = 0.5, alpha = 0.01)\n",
    "for cp in cluster_points:\n",
    "    ax.scatter3D(cp[:,0], cp[:,2], cp[:,1], s = 0.5, alpha = 0.25)\n",
    "median = np.median(points, axis=0)\n",
    "print('Median is %s'%median)\n",
    "ax.set_xlim3d(median[0]-thresh,median[0]+thresh)#min(sift_points[:,0]),min(sift_points[:,0])+max_dist)\n",
    "ax.set_ylim3d(median[2]-thresh,median[2]+thresh)#min(sift_points[:,1]),min(sift_points[:,1])+max_dist)\n",
    "ax.set_zlim3d(median[1]-50, median[1]+50)#min(sift_points[:,2]),min(sift_points[:,2])+max_dist)\n",
    "ax.view_init(elev=35., azim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find local descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_sizes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@param kpts: Numpy array (N, 2) with xy values of keypoints in image\n",
    "\"\"\"\n",
    "def resize_keypoints(kpts, img_dim, resize=256):\n",
    "    larger_axis = 0 if img_dim[0] > img_dim[1] else 1\n",
    "    larger_dim, smaller_dim = img_dim[larger_axis], img_dim[abs(1-larger_axis)]\n",
    "    empty_pixel = larger_dim-smaller_dim\n",
    "    lower_border = empty_pixel//2\n",
    "    upper_border = larger_dim - (empty_pixel // 2 + (empty_pixel % 2 > 0 ) )\n",
    "    valid =  [kpts[i,larger_axis] < upper_border and kpts[i,larger_axis] > lower_border for i in range(kpts.shape[0])]\n",
    "    if all(not x for x in valid):\n",
    "        return []\n",
    "    kpts = kpts[valid]\n",
    "    assert np.any(kpts[:,larger_axis] > lower_border), 'Lower border failed'\n",
    "    assert np.any(kpts[:,larger_axis] < upper_border), 'Upper border failed'\n",
    "    kpts[:,larger_axis] -= float(lower_border)\n",
    "    assert kpts.max() < smaller_dim, 'Crop gone wrong'\n",
    "    kpts *= float(resize-1)/float(smaller_dim)\n",
    "    return kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = cluster_orig_ids[0] #197"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Visualize resize of keypoints\n",
    "if 'orig_measurements' in locals():\n",
    "    kts_resized = kpts_0\n",
    "else:\n",
    "    kpts_0 = measurements[test_id]['kpts']\n",
    "    kpts_resized = resize_keypoints(kpts_0, img_sizes[test_id], resize=resolution)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(np.array(dataset[test_id][0]))\n",
    "if len(kpts_resized) > 0:\n",
    "    plt.scatter(kpts_resized[:,0], kpts_resized[:,1], s=0.25, alpha=0.9, c='red')\n",
    "plt.title('Resized/cropped image and keypoints')\n",
    "a = fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(np.array(real_size_dataset[test_id][0]), interpolation='bilinear')\n",
    "plt.scatter(kpts_0[:,0], kpts_0[:,1], s=0.25, alpha=0.9, c='red')\n",
    "plt.title('Original Image size and keypoints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not 'orig_measurements' in locals():\n",
    "    orig_measurements = measurements.copy()\n",
    "    for i, cam in enumerate(measurements):\n",
    "        if i % 1000 == 0:\n",
    "            print('%d/%d'%(i, len(measurements)))\n",
    "        measurements[cam]['kpts'] = resize_keypoints(measurements[cam]['kpts'], img_sizes[cam], resize=resolution)\n",
    "else:\n",
    "    print('already resized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some images have no corresponding points but then also are not matched to a cluster\n",
    "#for i in range(4328):\n",
    "#    if i not in measurements:\n",
    "#        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = np.array(query_imgs_high_res[query_id])\n",
    "query_kpts, query_des = sift.detectAndCompute(query_img, None)\n",
    "query_sift_img = cv2.drawKeypoints(query_img, query_kpts, None)\n",
    "med_kpt_size = np.median([x.size for x in query_kpts])\n",
    "print('Median keypoint size: %.2f'%med_kpt_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts = []\n",
    "#print(measurements[test_id]['kpts'])\n",
    "pts = measurements[test_id]['point_id']\n",
    "for i, kpt in enumerate(measurements[test_id]['kpts']):\n",
    "    kpts.append(cv2.KeyPoint(x=kpt[0], y=kpt[1], _size=med_kpt_size, _class_id=pts[i]))\n",
    "#print(len(kpts))\n",
    "kpts_sift, db_des = sift.compute(np.array(dataset[test_id][0]), kpts)\n",
    "dataset_sift_img=cv2.drawKeypoints(np.array(dataset[test_id][0]),kpts, None)\n",
    "dataset_sift_img_comp=cv2.drawKeypoints(np.array(dataset[test_id][0]),kpts_sift, None)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(dataset_sift_img)\n",
    "plt.axis('off')\n",
    "a = fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(dataset_sift_img_comp)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(query_sift_img)\n",
    "plt.axis('off')\n",
    "plt.title('Query Image')\n",
    "a = fig.add_subplot(1,2,2)\n",
    "plt.imshow(dataset_sift_img)\n",
    "plt.axis('off')\n",
    "plt.title('Global neighbor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "#matches = {}\n",
    "print(query_des.shape)\n",
    "print(db_des.shape)\n",
    "matches = matcher.knnMatch(db_des, query_des, k=2)\n",
    "# Need to draw only good matches, so create a mask\n",
    "#matchesMask = [[0,0] for i in range(len(m))]\n",
    "# ratio test as per Lowe's paper\n",
    "good = []\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        #matchesMask[i]=[1,0]\n",
    "        good.append(m)\n",
    "#m = m[matchesMask]\n",
    "matches = good\n",
    "        \n",
    "t = time.time() - t\n",
    "print('Matching took %d seconds'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.drawMatchesKnn expects list of lists as matches.\n",
    "#draw_params = dict(matchColor = (0,255,0),\n",
    "#                   singlePointColor = (255,0,0),\n",
    "#                   matchesMask = matchesMask,\n",
    "#                   flags = 2)\n",
    "img3 = np.empty((max(query_sift_img.shape[0], dataset_sift_img.shape[0]), query_sift_img.shape[1] + dataset_sift_img.shape[1], 3), dtype=np.uint8)\n",
    "cv2.drawMatches(np.array(dataset[cluster_orig_ids[0]][0]),kpts,query_img,query_kpts,matches,outImg=img3,matchColor=None, singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "plt.imshow(img3)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for g in good:\n",
    "#    print(g.queryIdx)\n",
    "kpts_good = [query_kpts[i] for i in [g.trainIdx for g in good]]\n",
    "dataset_sift_img_good=cv2.drawKeypoints(query_img,kpts_good, None)\n",
    "plt.imshow(dataset_sift_img_good)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_to_match = []\n",
    "key_point = []\n",
    "breaker = 0\n",
    "break_at = 10\n",
    "ratio_thresh = 0.99\n",
    "t = time.time()\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        kpts = []\n",
    "        point_ids = measurements[img]['point_id']\n",
    "        for i, kpt in enumerate(measurements[img]['kpts']):\n",
    "            kpts.append(cv2.KeyPoint(kpt[0], kpt[1], _size=med_kpt_size))\n",
    "        if len(kpts) <= 0:\n",
    "            continue\n",
    "        _, desc = sift.compute(np.array(dataset[img][0]), kpts)\n",
    "        \n",
    "        matcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "        matches = matcher.knnMatch(desc, query_des, k=2)\n",
    "        good = []\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < ratio_thresh*n.distance:\n",
    "                #matchesMask[i]=[1,0]\n",
    "                good.append(m)\n",
    "        kpts_good = [query_kpts[i] for i in [g.trainIdx for g in good]]\n",
    "        key_point += kpts_good\n",
    "        points_to_match += [point_ids[i] for i in [g.queryIdx for g in good]]\n",
    "        breaker += 1\n",
    "        if breaker < break_at:\n",
    "            fig = plt.figure(figsize=(10, 15))\n",
    "            a = fig.add_subplot(1, 2, 1)\n",
    "            plt.imshow(cv2.drawKeypoints(query_img,kpts_good, None))\n",
    "            data_img = np.array(dataset[img][0])\n",
    "            img3 = np.empty((max(query_sift_img.shape[0], data_img.shape[0]), query_sift_img.shape[1] + data_img.shape[1], 3), dtype=np.uint8)\n",
    "            cv2.drawMatches(data_img,kpts,query_img,query_kpts,good,outImg=img3,matchColor=None, singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "            a = fig.add_subplot(1, 2, 2)\n",
    "            plt.imshow(img3)\n",
    "            plt.show()\n",
    "t = time.time() - t\n",
    "print('Total matching time: %d seconds'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_points = points[points_to_match]\n",
    "print(matched_points.mean(axis=0))\n",
    "#matched_points[:,[2,1]] = matched_points[:,[1,2]]\n",
    "#print(matched_points.mean(axis=0))\n",
    "print(matched_points.shape)\n",
    "matched_keypoints = np.vstack([np.array([x.pt[0], x.pt[1]]) for x in key_point])\n",
    "print(matched_keypoints.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = path_to_queries[query_id].replace('data/AachenDayNight/images_upright/', '')\n",
    "cm = camera_matrices[query_path]\n",
    "camera_matrix = cm['cameraMatrix']\n",
    "distortion_coeff = cm['rad_dist']\n",
    "print(camera_matrix)\n",
    "print(distortion_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "dist_vec = np.array([distortion_coeff, 0, 0, 0])\n",
    "success, R_vec, translation, inliers = cv2.solvePnPRansac(\n",
    "        matched_points, matched_keypoints, camera_matrix, dist_vec,\n",
    "        iterationsCount=n_iter, reprojectionError=8.,\n",
    "        flags=cv2.SOLVEPNP_P3P)\n",
    "t = time.time() - t\n",
    "print('PnP RANSAC took %d seconds (%.2f/iteration)'%(t, float(t)/float(n_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    print('Successful matching')\n",
    "    print(inliers.shape)\n",
    "else:\n",
    "    print('Not succesful')\n",
    "print(R_vec)\n",
    "print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_points[inliers].shape)\n",
    "print(matched_keypoints[inliers].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_inliers = 10\n",
    "\n",
    "if success:\n",
    "    inliers = inliers[:, 0] if len(inliers.shape) > 1 else inliers\n",
    "    num_inliers = len(inliers)\n",
    "    inlier_ratio = len(inliers) / len(matched_keypoints)\n",
    "    success &= num_inliers >= min_inliers\n",
    "\n",
    "    ret, R_vec, t = cv2.solvePnP(\n",
    "                matched_points[inliers], matched_keypoints[inliers], camera_matrix,\n",
    "                dist_vec, rvec=R_vec, tvec=translation, useExtrinsicGuess=True,\n",
    "                flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    assert ret\n",
    "\n",
    "    query_T_w = np.eye(4)\n",
    "    query_T_w[:3, :3] = cv2.Rodrigues(R_vec)[0]\n",
    "    query_T_w[:3, 3] = t[:, 0]\n",
    "    w_T_query = np.linalg.inv(query_T_w)\n",
    "\n",
    "    #ret = LocResult(success, num_inliers, inlier_ratio, w_T_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_T_query)\n",
    "print('Result: %s'%w_T_query[:3, 3])\n",
    "if query_id > 3:\n",
    "    pose_stats_filename = os.path.join('data/AachenDayNight/', 'pose_stats.txt')\n",
    "    mean_t, std_t = np.loadtxt(pose_stats_filename)\n",
    "    position = dataset[dataset_queries[query_id-4]][1][:3]\n",
    "    position = position*std_t + mean_t\n",
    "    print('Groundtruth: %s'%str(position))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop notebook run all\n",
    "1.0/0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now all clusters\n",
    "t = time.time()\n",
    "kpts_des = {}\n",
    "place_lms = []\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        kpts = []\n",
    "        point_ids = measurements[img]['point_id']\n",
    "        place_lms += point_ids\n",
    "        for i, kpt in enumerate(measurements[img]['kpts']):\n",
    "            kpts.append(cv2.KeyPoint(kpt[0], kpt[1], _size=med_kpt_size))\n",
    "        if len(kpts) <= 0:\n",
    "            continue\n",
    "        kpts_des[img] = sift.compute(np.array(dataset[img][0]), kpts)\n",
    "place_lms = np.array(place_lms)\n",
    "t = time.time() - t\n",
    "print('Took %d seconds'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = list(cluster_query[0])[12]\n",
    "plt.imshow(cv2.drawKeypoints(np.array(dataset[random_id][0]),kpts_des[random_id][0], None))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kpts_des))\n",
    "print(sum([len(c) for c in cluster_query]))\n",
    "print(place_lms.shape)\n",
    "print(kpts_des[random_id][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://github.com/ethz-asl/hfnet\n",
    "\"\"\"\n",
    "\n",
    "def matches_cv2np(matches_cv):\n",
    "    matches_np = np.int32([[m.queryIdx, m.trainIdx] for m in matches_cv])\n",
    "    distances = np.float32([m.distance for m in matches_cv])\n",
    "    return matches_np.reshape(-1, 2), distances\n",
    "\n",
    "query_des = query_des.astype(np.float32, copy=False)\n",
    "dataset_des = np.concatenate([kpts_des[x][1] for x in kpts_des]).astype(np.float32, copy=False)\n",
    "#img_match_ids = [x for i in range(len(kpts_des[x][1])) for x in kpts_des]\n",
    "#print(len(img_match_ids))\n",
    "\n",
    "\n",
    "print(type(query_des))\n",
    "print(type(dataset_des))\n",
    "print(query_des.shape)\n",
    "print(dataset_des.shape)\n",
    "\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "m = matcher.knnMatch(dataset_des, query_des, k=2)\n",
    "\n",
    "matches1, matches2 = list(zip(*m))\n",
    "(matches1, dist1) = matches_cv2np(matches1)\n",
    "(matches2, dist2) = matches_cv2np(matches2)\n",
    "print(matches1.shape)\n",
    "print(matches2.shape)\n",
    "print('Done matching: %d matches'%len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_thresh = 0.70\n",
    "#print(matches1[:5])\n",
    "#print(matches2[:5])\n",
    "#print(matches1[103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(place_lms.shape)\n",
    "## Index testing cannot work as we do not know which points are in query (problem would be already solved)\n",
    "#good = (place_lms[matches1[:, 1]] == place_lms[matches2[:, 1]])\n",
    "good = (dist1/dist2 < ratio_thresh)\n",
    "matches_good = matches1[good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(good[:10])\n",
    "print(matches_good[0])\n",
    "#print(len([g for g in good if not g]))\n",
    "#print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "img_shown = 0\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        kpt = kpts_des[img][0]\n",
    "        n = len(kpt)\n",
    "        cnt += n\n",
    "        print('%d:%d'%(cnt-n,cnt))\n",
    "        if not np.any(good[cnt-n:cnt]):\n",
    "            continue\n",
    "        \n",
    "        img_shown += 1\n",
    "        if img_shown > 5:\n",
    "            break\n",
    "        #matches_img = [i for x in m[cnt-n:cnt] for i in x if x[0].distance < ratio_thresh*x[1].distance]\n",
    "        matches_img = [cv2.DMatch()]\n",
    "        data_img = np.array(dataset[img][0])\n",
    "        img3 = np.empty((max(query_sift_img.shape[0], data_img.shape[0]), query_sift_img.shape[1] + data_img.shape[1], 3), dtype=np.uint8)\n",
    "        cv2.drawMatches(data_img,kpt,query_img,query_kpts,matches_img,outImg=img3,matchColor=None, singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "        plt.imshow(img3)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_points = np.stack([points[place_lms[i]] for i in matches[:, 0]])\n",
    "matched_kpts = np.vstack([np.array(query_kpts[i].pt) for i in matches[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_points.shape)\n",
    "print(matched_kpts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 300\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(matched_points[:,0], matched_points[:,2], matched_points[:,1], s = 1.5, alpha = 0.5)\n",
    "#for cp in cluster_points:\n",
    "#    ax.scatter3D(cp[:,0], cp[:,2], cp[:,1], s = 0.5, alpha = 0.25)\n",
    "#ax.scatter3D(translation[0], translation[2], translation[1], s=25, alpha=1.0)\n",
    "median = np.median(points, axis=0)\n",
    "print('Median is %s'%median)\n",
    "#ax.set_xlim3d(median[0]-thresh,median[0]+thresh)#min(sift_points[:,0]),min(sift_points[:,0])+max_dist)\n",
    "#ax.set_ylim3d(median[2]-thresh,median[2]+thresh)#min(sift_points[:,1]),min(sift_points[:,1])+max_dist)\n",
    "#ax.set_zlim3d(median[1]-50, median[1]+50)#min(sift_points[:,2]),min(sift_points[:,2])+max_dist)\n",
    "ax.view_init(elev=35., azim=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfnet-pytorch",
   "language": "python",
   "name": "hfnet-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
