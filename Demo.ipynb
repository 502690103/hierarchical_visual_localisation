{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Model pipeline\n",
    "1. Find images with similar global descriptors\n",
    "2. Cluster by covisiblity (optional)\n",
    "3. Find local descriptors\n",
    "4. Match to SfM model\n",
    "5. Calculate pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports, Loading data\n",
    "Loading data into memory. This may take some minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import sqlite3\n",
    "#import read_model as rm\n",
    "import nearpy\n",
    "import threading\n",
    "from pyquaternion import Quaternion\n",
    "import transforms3d.quaternions as txq\n",
    "import matplotlib.colors as mcolors\n",
    "from argparse import Namespace\n",
    "\n",
    "from dataset_loaders.txt_to_db import get_images, get_points\n",
    "from dataset_loaders.utils import load_image\n",
    "from dataset_loaders.pose_utils import quaternion_angular_error\n",
    "import models.netvlad_vd16_pitts30k_conv5_3_max_dag as netvlad\n",
    "import models.demo_superpoint as superpoint\n",
    "import models.super_point_pytorch as superpoint_new\n",
    "from models.d2net.extract_features import d2net_interface\n",
    "from evaluate import *\n",
    "import common.feature_matching as matching\n",
    "from models.cirtorch_network import init_network, extract_vectors\n",
    "from common.epipolar import symmetric_epipolar_distance, expand_homo_ones\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#%matplotlib inline\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings\n",
    "global_method = 'Cirtorch'\n",
    "if global_method == 'NetVLAD':\n",
    "    global_resolution = 224  #alternative: 256\n",
    "elif global_method == 'Cirtorch':\n",
    "    global_resolution = 1024\n",
    "n_images = 5\n",
    "augmented = False\n",
    "clustering = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "if 'images' not in locals():\n",
    "    images = get_images()\n",
    "if 'points3d' not in locals():\n",
    "    points3d = get_points()\n",
    "t = time.time() - t\n",
    "print('Loaded data in {:.2f} seconds'.format(t))\n",
    "get_img_normal = lambda i: np.array(load_image('data/AachenDayNight/images_upright/{}'.format(images[i].name)))\n",
    "get_img_augmented = lambda i: load_image(os.path.join('data/AachenDayNight/AugmentedNightImages_v2',os.path.split(images[i].name)[-1]).replace('.jpg', '.png'))\n",
    "get_img = lambda i: get_img_normal(i) if i >= 0 else get_img_augmented(-i)\n",
    "database_cursor = get_cursor('data/AachenDayNight/aachen.db')\n",
    "query_cursor = get_cursor('data/queries.db')\n",
    "kpt_size = 1.\n",
    "##create image clusters\n",
    "if clustering:\n",
    "    t = time.time()\n",
    "    img_cluster = get_img_cluster(images, points3d)\n",
    "    t = time.time() - t\n",
    "    print('Found %d cluster in %d s'%(len(img_cluster), t))\n",
    "# Camera matrix\n",
    "camera_matrices = get_camera_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2 random day time queries\n",
    "2 random night time queries\n",
    "8 dataset queries\n",
    " - 4 randomly selected\n",
    " - 4 hard cases\n",
    "\"\"\"\n",
    "standard_queries = [1]#, 2500, 4000]#, 2849]\n",
    "#hard_examples = [2699, 2976, 294, 2907, 2435, 2705, 2670, 3061, 2675, 2672, 3239, 2566,2322,2673,2678, 2053, 3784]\n",
    "#dataset_queries += hard_examples\n",
    "no_global = [2907, 3239, 2786, 3786, 3500, 3970, 3966, 4083]\n",
    "few_inliers = [2976, 294, 2435, 2675, 2678, 1246, 3784, 2048, 3788, 3790, 3839, 3849, 382, 2874, 258, 634, 3054, 3055, 160, 162, 421, 1236, 559, 2426, 2428, 557, 3474, 371, 2066, 3938, 223, 3445, 615, 2606, 572, 2593, 4043, 2300]\n",
    "other = [2699,  2705,  3061,  2672,  2566,  2673,  2053,  3791,  2885,  1930,  1228,  2325,  2580,  1401,  4050,  4051,  4048]\n",
    "other_no_refilter = [2699,  2705,  2670,  2322,  2885,  3802,  4032,  4026,  421,  1228,  2504,  2580,  572,  4050,  4051,  4048,  1926]\n",
    "augmented_queries = []# [1, 500, 250]\n",
    "mixed = standard_queries[0:4] + [few_inliers[i] for i in [1,-1]] + [other[i] for i in[-1,6]]\n",
    "dataset_queries = standard_queries #+ no_global\n",
    "path_to_queries = [\n",
    "                   # 'data/AachenDayNight/images_upright/query/day/nexus4/IMG_20130210_165147.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/query/day/nexus5x/IMG_20161227_162905.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/query/day/nexus5x/IMG_20161227_160713.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/night/nexus5x/IMG_20161227_172616.jpg',\n",
    "                   'data/AachenDayNight/images_upright/query/night/nexus5x/IMG_20161227_191152.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/db/1.jpg',\n",
    "                   #'data/AachenDayNight/images_upright/db/500.jpg'\n",
    "                  ] \n",
    "path_to_queries = []\n",
    "num_query_images = len(path_to_queries)\n",
    "path_to_queries = path_to_queries + ['data/AachenDayNight/images_upright/db/'+str(i)+'.jpg' for i in dataset_queries] \n",
    "path_to_queries = path_to_queries + ['data/AachenDayNight/AugmentedNightImages_high_res/'+str(i)+'.png' for i in augmented_queries]\n",
    "n_queries = len(path_to_queries)\n",
    "low_res_transform = transforms.Compose([transforms.Resize(global_resolution), transforms.CenterCrop(global_resolution)])\n",
    "query_imgs_high_res = [load_image(path) for path in path_to_queries]\n",
    "#query_imgs_low_res = [transform(img) for img in query_imgs_high_res]\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "plt.title('Queries')\n",
    "plt.axis('off')\n",
    "for i, img in enumerate(query_imgs_high_res):\n",
    "    a = fig.add_subplot(1, len(query_imgs_high_res), i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find similar images (global descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_method == 'NetVLAD':\n",
    "    print('use netvlad')\n",
    "    model = netvlad.vd16_pitts30k_conv5_3_max_dag(weights_path='data/teacher_models/netvlad_pytorch/vd16_pitts30k_conv5_3_max_dag.pth')\n",
    "    model.eval()\n",
    "    print('set up netvlad')\n",
    "    query_global_desc = [model(transforms.ToTensor()(low_res_transform(img)).unsqueeze(0)).detach().cpu().squeeze(0).numpy() for img in query_imgs_high_res]\n",
    "    query_global_desc = np.vstack(query_global_desc)\n",
    "elif global_method == 'Cirtorch':\n",
    "    print('use gem')\n",
    "    state = torch.load('data/teacher_models/retrievalSfM120k-resnet101-gem-b80fb85.pth')   \n",
    "    net_params = {}\n",
    "    net_params['architecture'] = state['meta']['architecture']\n",
    "    net_params['pooling'] = state['meta']['pooling']\n",
    "    net_params['local_whitening'] = state['meta'].get('local_whitening', False)\n",
    "    net_params['regional'] = state['meta'].get('regional', False)\n",
    "    net_params['whitening'] = state['meta'].get('whitening', False)\n",
    "    net_params['mean'] = state['meta']['mean']\n",
    "    net_params['std'] = state['meta']['std']\n",
    "    net_params['pretrained'] = False\n",
    "    # load network\n",
    "    net = init_network(net_params)\n",
    "    net.load_state_dict(state['state_dict'])\n",
    "    if 'Lw' in state['meta']:\n",
    "        net.meta['Lw'] = state['meta']['Lw']\n",
    "    ms = list(eval('[1]'))\n",
    "    if len(ms)>1 and net.meta['pooling'] == 'gem' and not net.meta['regional'] and not net.meta['whitening']:\n",
    "        msp = net.pool.p.item()\n",
    "        print(\">> Set-up multiscale:\")\n",
    "        print(\">>>> ms: {}\".format(ms))            \n",
    "        print(\">>>> msp: {}\".format(msp))\n",
    "    else:\n",
    "        msp = 1\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "    # set up the transform\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=net.meta['mean'],\n",
    "        std=net.meta['std']\n",
    "    )\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    Lw = None\n",
    "    print('set up gem')\n",
    "    query_global_desc = extract_vectors(net, path_to_queries, global_resolution, transform, ms=ms, msp=msp)\n",
    "    query_global_desc = query_global_desc.numpy().T\n",
    "print('{} query descriptors with {} dimensions'.format(*query_global_desc.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global descriptors for dataset are precalculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading database global features')\n",
    "t = time.time()\n",
    "if global_method == 'NetVLAD':\n",
    "    if augmented:\n",
    "        global_features_cursor = get_cursor('data/global_features_augmented.db')\n",
    "    else:\n",
    "        global_features_cursor = get_cursor('data/global_features_low_res.db')\n",
    "\n",
    "    global_features = []\n",
    "    image_ids = []\n",
    "    for row in global_features_cursor.execute('SELECT image_id, cols, data FROM global_features;'):\n",
    "        global_features.append(np.frombuffer(row[2], dtype=np.float32).reshape(-1, row[1]))\n",
    "        image_ids.append(row[0])\n",
    "    if augmented:    \n",
    "        for row in global_features_cursor.execute('SELECT image_id, cols, data FROM global_augmented_features;'):\n",
    "            global_features.append(np.frombuffer(row[2], dtype=np.float32).reshape(-1, row[1]))\n",
    "            image_ids.append(-row[0])\n",
    "\n",
    "    global_features = np.vstack(global_features)\n",
    "    global_features_cursor.close()\n",
    "elif global_method == 'Cirtorch':\n",
    "    global_features = np.load('data/cirtorch_data_descs.npy').T\n",
    "    image_ids = [images[i].id for i in images]\n",
    "    if augmented:\n",
    "        aug_features = np.load('data/cirtorch_augmented_descs.npy').T\n",
    "        global_features = np.concatenate([global_features, aug_features])\n",
    "        image_ids += [-images[i].id for i in images]\n",
    "print('{} dataset descriptors with {} dimensions'.format(*global_features.shape))\n",
    "t = time.time() - t\n",
    "print('%d seconds'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_neighbor_method = 'approx_cpu' #choices=['LSH', 'exact', 'approx']\n",
    "global_matcher = matching.GlobalMatcher(global_neighbor_method, n_images+2 if augmented else n_images+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = global_matcher.match(query_global_desc, global_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discard direct neighbors of dataset images (i.e. themselves)\n",
    "indices_cut = []\n",
    "for i in range(num_query_images):\n",
    "    indices_cut.append(indices[i,:n_images])\n",
    "for i in range(num_query_images, len(path_to_queries)):\n",
    "    #indices_cut.append(indices[i,1:n_images+1])\n",
    "    lst = []\n",
    "    j = 0\n",
    "    img_itself = abs(image_ids[indices[i,0]])\n",
    "    for idx in indices[i,1:]:\n",
    "        #aug = image_ids[idx] < 0\n",
    "        #ixs = -image_ids[idx] if aug else image_ids[idx]\n",
    "        if abs(image_ids[idx]) != img_itself:\n",
    "            lst.append(idx)\n",
    "            j += 1\n",
    "        if j >= n_images:\n",
    "            break\n",
    "    indices_cut.append(lst)\n",
    "    #print(len(lst))\n",
    "indices = np.stack(indices_cut)\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_img_name(cursor, i):\n",
    "    n, = next(cursor.execute('SELECT name FROM images WHERE image_id=?;',(i,)))\n",
    "    return n\n",
    "\n",
    "def calc_neighbor_match_ipynb(img_idx, neighbor_idx, images):\n",
    "    oimg = images[img_idx]\n",
    "    nimg = images[neighbor_idx]\n",
    "    print(oimg.tvec)\n",
    "    print(nimg.tvec)\n",
    "\n",
    "    valid_o = oimg.point3D_ids > 0 \n",
    "    pt_ids_o = oimg.point3D_ids[valid_o]\n",
    "\n",
    "    valid_n = nimg.point3D_ids > 0 \n",
    "    pt_ids_n = nimg.point3D_ids[valid_n]\n",
    "\n",
    "    shared = np.isin(pt_ids_o, pt_ids_n)\n",
    "    pt_ids_s = pt_ids_o[shared]\n",
    "    #print('Images match {:.1f}%'.format(100.0*(pt_ids_s.shape[0]/pt_ids_o.shape[0])))\n",
    "    return 100.0*(pt_ids_s.shape[0]/min([pt_ids_o.shape[0], pt_ids_n.shape[0]]))\n",
    "\n",
    "query_id = 3\n",
    "\n",
    "in1 = 'db/'+path_to_queries[query_id].split('/')[-1].replace('.png', '.jpg')\n",
    "in2 = images[image_ids[indices[query_id,0]]].name\n",
    "print(in1)\n",
    "print(in2)\n",
    "id1 = get_img_id(database_cursor, in1)\n",
    "id2 = get_img_id(database_cursor, in2)\n",
    "print(id1)\n",
    "print(id2)\n",
    "print(get_img_name(database_cursor, id1))\n",
    "print(get_img_name(database_cursor, id2))\n",
    "print(calc_neighbor_match_ipynb(id1, id2, images))\n",
    "print(calc_neighbor_match_ipynb(id2, id1, images))\n",
    "p1 = colmap_image_to_pose(images[id1])[:3,3]\n",
    "p2 = colmap_image_to_pose(images[id2])[:3,3]\n",
    "fig = plt.figure(figsize=(25,10), constrained_layout=True)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(get_img(id1))\n",
    "ax.axis('off')\n",
    "ax.set_title('Pos: {}'.format(p1), fontsize=28)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "fig.suptitle('Distance: {:.1f}m'.format(np.linalg.norm(p1 - p2)), fontsize=28)\n",
    "ax.imshow(get_img(id2))\n",
    "ax.axis('off')\n",
    "ax.set_title('Pos: {}'.format(p2), fontsize=28)\n",
    "fig.savefig('figures/colmap_global_error_{}.png'.format(query_id))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal = True\n",
    "\n",
    "max_n = min(5, n_images)\n",
    "\n",
    "if horizontal:\n",
    "    fig = plt.figure(figsize=(30,50))\n",
    "else:\n",
    "    fig = plt.figure(figsize=(50,30))\n",
    "for i, query_img in enumerate(query_imgs_high_res):\n",
    "    imgs = [query_img] \n",
    "    for n, j in enumerate(indices[i]):\n",
    "        if n < max_n:\n",
    "            if j > len(images):\n",
    "                imgs = imgs + [get_img_augmented(-image_ids[j])]\n",
    "            else:\n",
    "                imgs = imgs + [get_img(image_ids[j])]\n",
    "    #plt.title('Neighbors')\n",
    "    img_name = 'db/'+path_to_queries[i].split('/')[-1].replace('.png', '.jpg')\n",
    "    for j, img in enumerate(imgs):\n",
    "        # use this to transpose figure\n",
    "        if horizontal:\n",
    "            ax = fig.add_subplot(n_images+1, n_queries, j*len(query_imgs_high_res)+i+1)\n",
    "        else:\n",
    "            ax = fig.add_subplot(n_queries, max_n+1, i*(max_n+1)+j+1)\n",
    "        ax.imshow(img)\n",
    "        if i >= num_query_images and j > 0:\n",
    "            #if augmented:\n",
    "            #    pass\n",
    "            #else:\n",
    "            #ax.set_title('{:.1f}% Match'.format(calc_neighbor_match(get_img_id(database_cursor, img_name), abs(image_ids[indices[i,j-1]]), images)))\n",
    "        #    plt.title('Neighbor')#'%.0f'%distances[i][j-1])\n",
    "            pass\n",
    "        elif j == 0:\n",
    "            #ax.set_title(i)\n",
    "            pass\n",
    "        ax.axis('off')\n",
    "#fig.savefig('figures/global_neighbor_fails.png', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "horizontal = True\n",
    "\n",
    "max_n = min(3, n_images)\n",
    "\n",
    "if horizontal:\n",
    "    fig = plt.figure(figsize=(30,50))\n",
    "else:\n",
    "    fig = plt.figure(figsize=(50,30))\n",
    "for i, query_img in enumerate(query_imgs_high_res):\n",
    "    imgs = []#query_img] \n",
    "    for n, j in enumerate(indices[i]):\n",
    "        if n < max_n:\n",
    "            if j > len(images):\n",
    "                imgs = imgs + [get_img_augmented(-image_ids[j])]\n",
    "            else:\n",
    "                imgs = imgs + [get_img(image_ids[j])]\n",
    "    #plt.title('Neighbors')\n",
    "    img_name = 'db/'+path_to_queries[i].split('/')[-1].replace('.png', '.jpg')\n",
    "    for j, img in enumerate(imgs):\n",
    "        # use this to transpose figure\n",
    "        if horizontal:\n",
    "            ax = fig.add_subplot(n_images, n_queries, j*len(query_imgs_high_res)+i+1)\n",
    "        else:\n",
    "            ax = fig.add_subplot(n_queries, max_n, i*(max_n+1)+j+1)\n",
    "        ax.imshow(img)\n",
    "        if i >= num_query_images and j > 0:\n",
    "            #if augmented:\n",
    "            #    pass\n",
    "            #else:\n",
    "            #ax.set_title('{:.1f}% Match'.format(calc_neighbor_match(get_img_id(database_cursor, img_name), abs(image_ids[indices[i,j-1]]), images)))\n",
    "        #    plt.title('Neighbor')#'%.0f'%distances[i][j-1])\n",
    "            pass\n",
    "        elif j == 0:\n",
    "            #ax.set_title(i)\n",
    "            pass\n",
    "        ax.axis('off')\n",
    "#fig.savefig('figures/global_neighbor_fails.png', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_good_neighbor = []\n",
    "n_good_neighbors = []\n",
    "for query_id in range(num_query_images, len(query_imgs_high_res)):\n",
    "    query_db_id = get_img_id(database_cursor, 'db/'+path_to_queries[query_id].split('/')[-1].replace('.png', '.jpg'))\n",
    "    found = False\n",
    "    n, k = 0, 0\n",
    "    for i, idx in enumerate(indices[query_id]):\n",
    "        m = calc_neighbor_match(query_db_id, abs(image_ids[idx]), images)\n",
    "        #print(m)\n",
    "        if m > 0.0:\n",
    "            if not found:\n",
    "                first_good_neighbor.append(i)\n",
    "                k = i\n",
    "            n += 1\n",
    "            #plt.imshow(get_img(image_ids[idx]))\n",
    "            #plt.show()\n",
    "            found = True\n",
    "    print('id: {}\\tk: {}\\tn: {}'.format(query_id, k, n))\n",
    "    n_good_neighbors.append(n)\n",
    "            \n",
    "    if not found:\n",
    "        first_good_neighbor.append(-1)\n",
    "        print('id: {}\\tNo match found in {} neighbors'.format(query_id, n_images))\n",
    "        \n",
    "print(first_good_neighbor)\n",
    "print(n_good_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "#indices_global = __approx_np_global__(global_features[:global_features.shape[0]//2] if augmented else global_features, global_features,global_features.shape[0]//2-1 if augmented else global_features.shape[0]-1)  # cpu version\n",
    "#indices_global = __approx_torch_global__(global_features[:global_features.shape[0]//2] if augmented else global_features, global_features,global_features.shape[0]//2 if augmented else global_features.shape[0])# gpu version\n",
    "global_matcher = matching.GlobalMatcher(global_neighbor_method, global_features.shape[0]//2-1 if augmented else global_features.shape[0]-1)\n",
    "indices_global = global_matcher.match(global_features[:global_features.shape[0]//2] if augmented else global_features, global_features)\n",
    "t = time.time() - t\n",
    "print('{}'.format(time_to_str(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "nb_lookup = {}\n",
    "for i, nbs in enumerate(indices_global):\n",
    "    k = 1\n",
    "    while k < nbs.shape[0] and calc_neighbor_match(image_ids[i], abs(image_ids[nbs[k]]), images) <= 1.0:\n",
    "        k += 1\n",
    "    #print('id: {}\\tk: {}\\t Match: {:.1f}'.format(i, k, calc_neighbor_match(image_ids[i], image_ids[indices[i,k]])))\n",
    "    nb_lookup[i] = k\n",
    "t = time.time() - t\n",
    "print('{:.1f} seconds'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array(list(nb_lookup.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect(vals, k):\n",
    "    return vals[vals > k].shape[0]\n",
    "def get_correct(vals, k):\n",
    "    return vals[vals <= k].shape[0]/vals.shape[0]\n",
    "k = 20\n",
    "wrong = get_incorrect(vals, k)\n",
    "print('{}/{} ({:.2f}%) have incorrect global neighbor for k={}'.format(wrong, vals.shape[0], 100.0*(wrong/vals.shape[0]), k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)\n",
    "plt.plot(x, np.ones_like(x))\n",
    "plt.plot(x, [get_correct(vals, i) for i in x])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Rate of imgs with at least one correct neighbor')\n",
    "plt.savefig('figures/global_matching_rate.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_hist = vals.copy()\n",
    "cut_value = 20\n",
    "vals_hist[vals_hist > cut_value] = cut_value\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(vals_hist, bins=cut_value, log=True)\n",
    "ax.set_ylabel('Number of images (log scale)')\n",
    "ax.set_xlabel('k')\n",
    "fig.canvas.draw()\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels[-2] = '>={}'.format(cut_value)\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig('figures/k_hist.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covisibility clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 0\n",
    "print('Query {}/{}'.format(query_id+1, len(query_imgs_high_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_query = [[image_ids[indices[query_id][i]] for i in range(n_images)]] ## augmentation currently not supported for local matching\n",
    "cluster_orig_ids = [image_ids[indices[query_id][0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clustering:\n",
    "    cluster_query = [img_cluster[image_ids[indices[query_id][0]]]]\n",
    "    cluster_orig_ids = [image_ids[indices[query_id][0]]]\n",
    "    for i, ind in enumerate(indices[query_id]):\n",
    "        ind = image_ids[ind]\n",
    "        if i == 0:\n",
    "            continue\n",
    "        point_set = img_cluster[ind]\n",
    "        print('Match neighbor %d'%i)\n",
    "        disjoint = False\n",
    "        for j, c in enumerate(cluster_query):\n",
    "            if ind in c:\n",
    "                print('  - Can be matched to cluster')\n",
    "                #print(point_set - (point_set-cluster))\n",
    "                cluster_query[j] |= point_set\n",
    "                disjoint = True\n",
    "                break\n",
    "        if not disjoint:\n",
    "            print('  - New cluster created')\n",
    "            cluster_orig_ids.append(ind)\n",
    "            cluster_query.append(point_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} images in all cluster'.format(sum([len(i) for i in cluster_query])))\n",
    "print('Global neighbor ids: {}'.format([i for j in cluster_query for i in j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs_per_cluster = 5\n",
    "num_imgs_per_cluster = min(num_imgs_per_cluster, n_images)\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "#plt.title('Covisibility clustering')\n",
    "#plt.axis('off')\n",
    "for i, cluster in enumerate(cluster_query):\n",
    "    imgs = list(cluster)\n",
    "    sh = np.arange(len(imgs))\n",
    "    np.random.shuffle(sh)\n",
    "    sh = sh[:num_imgs_per_cluster]\n",
    "    imgs = np.array(imgs)[sh]\n",
    "    imgs = [get_img(cluster_orig_ids[i])]+[get_img(j) for j in imgs]\n",
    "    for j, img in enumerate(imgs):\n",
    "        a = fig.add_subplot(len(cluster_query), len(imgs), i*len(imgs)+j+1)\n",
    "        a.imshow(img)\n",
    "        a.axis('off')\n",
    "        if j == 0:\n",
    "            plt.title('Global neighbor')\n",
    "#fig.savefig('figures/cluster.png', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Create scatter plot of 3d points in images')\n",
    "cluster_points = []\n",
    "for i, c in enumerate(cluster_query):\n",
    "    imgs_cluster = list(c)\n",
    "    points_cluster = set()\n",
    "    for ind in imgs_cluster:\n",
    "        pt_ids = images[abs(ind)].point3D_ids\n",
    "        valid = pt_ids > 0\n",
    "        points_cluster |= set(pt_ids[valid])\n",
    "    points_cluster = list(points_cluster)\n",
    "    cluster_points.append(np.stack([points3d[x].xyz for x in points_cluster]))\n",
    "    \"\"\"mask = np.ones(len(points3d),dtype=bool) #np.ones_like(a,dtype=bool)\n",
    "    mask[points_cluster] = False\n",
    "\n",
    "    cluster_points.append(points3d[~mask])\"\"\"\n",
    "    print('%d\\tpoints in cluster'%cluster_points[i].shape[0])\n",
    "#other_points = points[mask]\n",
    "#print('%d other points'%other_points.shape[0])\n",
    "thresh = 300\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.scatter3D(other_points[:,0], other_points[:,2], other_points[:,1], s = 0.5, alpha = 0.01)\n",
    "for cp in cluster_points:\n",
    "    ax.scatter3D(cp[:,0], cp[:,2], cp[:,1], s = 0.5, alpha = 0.25)\n",
    "median = np.sum([np.median(cp, axis=0)*cp.shape[0] for cp in cluster_points], axis=0)/float(sum([cp.shape[0] for cp in cluster_points]))\n",
    "print('Median is %s'%median)\n",
    "ax.set_xlim3d(median[0]-thresh,median[0]+thresh)#min(sift_points[:,0]),min(sift_points[:,0])+max_dist)\n",
    "ax.set_ylim3d(median[2]-thresh,median[2]+thresh)#min(sift_points[:,1]),min(sift_points[:,1])+max_dist)\n",
    "ax.set_zlim3d(median[1]-50, median[1]+50)#min(sift_points[:,2]),min(sift_points[:,2])+max_dist)\n",
    "ax.view_init(elev=35., azim=0)\n",
    "#plt.savefig('figures/cluster_3d.png', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_std = 1.0\n",
    "img_idx = 12\n",
    "\n",
    "def plt_pointcloud(img_idx, ax):\n",
    "    valid = images[img_idx].point3D_ids > 0\n",
    "    pt_ids = images[img_idx].point3D_ids[valid]\n",
    "    pts = np.stack([points3d[i].xyz for i in pt_ids])\n",
    "    print(pts.shape)\n",
    "    mean, std = np.mean(pts, axis=0), np.std(pts, axis=0)\n",
    "    print(std)\n",
    "    print(np.std(std))\n",
    "    print(mean.shape)\n",
    "    if within_std > 0:\n",
    "        valid_pts = np.all(np.abs(pts - mean) < within_std*std, axis=1)\n",
    "        print(valid_pts.shape)\n",
    "        pts = pts[valid_pts]\n",
    "    print(pts.shape)\n",
    "    ax.scatter3D(pts[:,0], pts[:,2], pts[:,1], c='r', s=1)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "plt_pointcloud(img_idx, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find local descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Order to get image - keypoint - 3D point correspondances: </b>\n",
    "1. take image\n",
    "2. find image name\n",
    "3. find db_id of image_name\n",
    "4. get keypoints\n",
    "5. get valid 3d points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_id >= num_query_images:\n",
    "    nn = first_good_neighbor[query_id-num_query_images]\n",
    "else:\n",
    "    nn = 0\n",
    "test_id = cluster_query[0][nn] if nn >= 0 else cluster_orig_ids[0]\n",
    "#test_id = cluster_orig_ids[0] #197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_id < num_query_images:\n",
    "    test_query_path = path_to_queries[query_id].replace('data/AachenDayNight/images_upright/query/', '')\n",
    "    print(test_query_path)\n",
    "    query_img_id = get_img_id(query_cursor, test_query_path)\n",
    "    query_kpts, query_desc_colmap = get_kpts_desc(query_cursor, query_img_id)\n",
    "else:\n",
    "    fake_query_path = 'db/'+path_to_queries[query_id].split('/')[-1].replace('.png','.jpg')\n",
    "    print(fake_query_path)\n",
    "    query_img_id = get_img_id(database_cursor,fake_query_path)\n",
    "    query_kpts, query_desc_colmap = get_kpts_desc(database_cursor, query_img_id)\n",
    "    \n",
    "query_kpts_colmap = kpts_to_cv(query_kpts, kpt_size=30.0)\n",
    "query_img = cv2.drawKeypoints(np.array(query_imgs_high_res[query_id]),query_kpts_colmap, None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(query_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_id < num_query_images:\n",
    "    db_id = [i for j in cluster_query for i in j][1]\n",
    "else:\n",
    "    db_id = [i for j in cluster_query for i in j][first_good_neighbor[query_id]] #cluster_orig_ids[0]\n",
    "img_name = images[abs(db_id)].name\n",
    "if db_id >= 0:\n",
    "    neighbor_img_original_path = 'data/AachenDayNight/images_upright/'+img_name\n",
    "elif db_id <= 0:\n",
    "    neighbor_img_original_path = 'data/AachenDayNight/AugmentedNightImages_high_res/'+os.path.split(img_name)[-1].replace('.jpg', '.png')\n",
    "neighbor_img_original = np.array(load_image(neighbor_img_original_path))\n",
    "print(img_name)\n",
    "valid = images[abs(db_id)].point3D_ids > 0 \n",
    "#print([i for i in range(len(valid)) if valid[i]])\n",
    "neighbor_kpts, neighbor_desc_colmap = get_kpts_desc(database_cursor, abs(db_id))\n",
    "neighbor_kpts = neighbor_kpts[valid[:neighbor_kpts.shape[0]]] - 0.5\n",
    "neighbor_kpts_numpy = neighbor_kpts.copy()\n",
    "neighbor_kpts_cv = kpts_to_cv(neighbor_kpts, kpt_size=100.)\n",
    "pt_ids = images[abs(db_id)].point3D_ids[valid]\n",
    "print('len kpts: %d'%len(neighbor_kpts_cv))\n",
    "plt.figure(figsize=(10,10))\n",
    "neighbor_img = cv2.drawKeypoints(neighbor_img_original,neighbor_kpts_cv, None)\n",
    "plt.imshow(neighbor_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "neighbor_desc_colmap = neighbor_desc_colmap[valid[:neighbor_desc_colmap.shape[0]]]\n",
    "#print(images[db_id])\n",
    "#print(pt_ids.shape)\n",
    "#print(len(neighbor_kpts_cv))\n",
    "#print(len(valid)//2)\n",
    "#print(neighbor_desc_colmap.shape)\n",
    "#print(neighbor_img_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "a = fig.add_subplot(121)\n",
    "a.imshow(neighbor_img_original)\n",
    "a.scatter(neighbor_kpts_numpy[:,0], neighbor_kpts_numpy[:,1], c='r', s=.75)\n",
    "a.axis('off')\n",
    "valid = images[db_id].point3D_ids > 0\n",
    "pt_ids = images[db_id].point3D_ids[valid]\n",
    "pts = np.stack([points3d[i].xyz for i in pt_ids])\n",
    "a = fig.add_subplot(122, projection='3d')\n",
    "plt_pointcloud(db_id, a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ['d2_tf_no_phototourism.pth' ,'d2_ots.pth' ,'d2_tf.pth'][1]\n",
    "d2net = d2net_interface(model_file=os.path.join('data/teacher_models/d2net/', model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2net_query_kpts, d2net_query_descs, d2net_scores =  d2net.extract_features(path_to_queries[query_id], only_path=True)\n",
    "if 'ots' in model_name or 'no_photo' in model_name:\n",
    "    d2net_query_kpts = d2net_query_kpts[0]\n",
    "    d2net_query_descs = d2net_query_descs[0]\n",
    "    d2net_scores = d2net_scores[0]\n",
    "d2net_query_kpts = d2net_query_kpts[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d2net_query_kpts.shape)\n",
    "print(d2net_query_kpts.max(axis=0))\n",
    "print(neighbor_img_original.shape)\n",
    "print(neighbor_kpts_numpy.max(axis=0))\n",
    "print(d2net_query_descs.shape)\n",
    "print(d2net_scores.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = d2net_query_kpts[:,0:2]\n",
    "print(test.shape)\n",
    "print(test.max(axis=0))\n",
    "test = np.flip(test, axis=1)\n",
    "print(test.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_kpts_cv = kpts_to_cv(d2net_query_kpts, kpt_size=100.)\n",
    "qi = cv2.drawKeypoints(np.array(query_imgs_high_res[query_id]),d2_kpts_cv, None)\n",
    "plt.imshow(qi)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2net_neighbor_descs = d2net.get_features(neighbor_img_original_path, np.flip(neighbor_kpts_numpy, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = True\n",
    "if my_config:\n",
    "    extractor = superpoint.SuperPointFrontend(weights_path='data/teacher_models/superpoint_v1.pth',\n",
    "                          nms_dist=4, conf_thresh=0.015, nn_thresh=.7, cuda=torch.cuda.is_available())\n",
    "else:\n",
    "    config = {\n",
    "                'nms_radius': 0,\n",
    "                'detector_threshold': 0,\n",
    "                'num_keypoints': 2000,\n",
    "                'border_remove': 4,\n",
    "            }\n",
    "    extractor = superpoint_new.SuperPointFrontend(config, weights_path='data/teacher_models/superpoint_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def rgb2gray(rgb):\n",
    "#    rgb = np.dot(rgb[...,:3], [0.299, 0.587, 0.144]).astype(np.float32) \n",
    "#    return rgb / rgb.max()\n",
    "#example_grayscale = rgb2gray(np.array(query_imgs_high_res[query_id]))\n",
    "\n",
    "if my_config:\n",
    "    example_grayscale = cv2.imread(path_to_queries[query_id], 0).astype(np.float32)/255.0\n",
    "else:\n",
    "    query_img_original = np.array(query_imgs_high_res[query_id],dtype=np.float32)\n",
    "    example_grayscale = query_img_original[:, :, 0]/255.0\n",
    "print(example_grayscale.shape)\n",
    "\n",
    "plt.imshow(example_grayscale, cmap = plt.get_cmap('gray'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()  \n",
    "if my_config:\n",
    "    pts, query_desc_superpoint, heatmap = extractor.run(example_grayscale)\n",
    "    query_desc_superpoint = query_desc_superpoint.T\n",
    "    pts = pts.T\n",
    "    query_kpts_numpy = pts.copy()[:,0:2]\n",
    "else:\n",
    "    ret = extractor.run(query_img_original)\n",
    "    query_desc_superpoint = ret['local_descriptors']\n",
    "    pts = ret['keypoints']\n",
    "    print(pts.shape)\n",
    "print(query_desc_superpoint.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pts.shape)  #3xN numpy array with corners [x_i, y_i, confidence_i]^T\n",
    "#print(query_desc_superpoint.shape) #256xN numpy array of corresponding unit normalized descriptors.\n",
    "#print(heatmap.shape)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2 if my_config else 1, 1)\n",
    "query_kpts_superpoint = kpts_to_cv(pts, kpt_size=50.0)\n",
    "query_img = cv2.drawKeypoints(np.array(query_imgs_high_res[query_id]),query_kpts_superpoint, None)\n",
    "plt.imshow(query_img)\n",
    "a.set_title('Query image with keypoints')\n",
    "plt.axis('off')\n",
    "if my_config:\n",
    "    a = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(heatmap)\n",
    "    a.set_title('Heatmap')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if my_config:\n",
    "    neighbor_img_original_gs = cv2.imread(neighbor_img_original_path, 0).astype(np.float32)/255.0\n",
    "    _, neighbor_desc_superpoint, heatmap = extractor.run(neighbor_img_original_gs, points=neighbor_kpts)\n",
    "    neighbor_desc_superpoint = neighbor_desc_superpoint.T\n",
    "else:\n",
    "    ret_neighbor = extractor.run(neighbor_img_original.astype(np.float32)/255.0)\n",
    "    desc_map = ret_neighbor['local_descriptor_map']\n",
    "    k = (neighbor_kpts.copy()//8).astype(np.int)\n",
    "    k = np.flip(k, axis=1)\n",
    "    neighbor_desc_superpoint = desc_map[k[:,0],k[:,1]]\n",
    "    \n",
    "    \n",
    "\n",
    "print('Superpoint descriptor has {} features with {} dimensions'.format(*neighbor_desc_superpoint.shape))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(query_img)\n",
    "plt.axis('off')\n",
    "plt.title('Query Image')\n",
    "a = fig.add_subplot(1,2,2)\n",
    "plt.imshow(neighbor_img)\n",
    "plt.axis('off')\n",
    "plt.title('Global neighbor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_type = 'approx_torch' # choices = ['OpenCV', 'approx_torch', 'approx']\n",
    "ratio_thresh = .75\n",
    "local_matcher = matching.LocalMatcher(ratio_thresh, matching_type, False)\n",
    "#local_matcher = matching.GlobalMatcher('approx', 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'Superpoint'    # choices = ['colmap', 'superpoint']\n",
    "if method == 'Colmap':\n",
    "    query_desc = query_desc_colmap\n",
    "    neighbor_desc = neighbor_desc_colmap\n",
    "    query_kpts = query_kpts_colmap\n",
    "elif method == 'Superpoint':\n",
    "    query_desc = query_desc_superpoint\n",
    "    neighbor_desc = neighbor_desc_superpoint\n",
    "    query_kpts = query_kpts_superpoint\n",
    "elif method == 'D2':\n",
    "    query_desc = d2net_query_descs\n",
    "    query_kpts = d2_kpts_cv\n",
    "    neighbor_desc = d2net_neighbor_descs\n",
    "else:\n",
    "    raise NotImplementedError('Method not implemented')\n",
    "\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "print(query_desc.shape)\n",
    "print(neighbor_desc.shape)\n",
    "#\"\"\"\n",
    "def double_matching(local_matcher, query_desc, neighbor_desc):\n",
    "    matches_forward = local_matcher.match(query_desc, neighbor_desc)\n",
    "    matches_reverse = local_matcher.match(neighbor_desc, query_desc)\n",
    "    if matches_forward.shape[0] == 0 or matches_reverse.shape[0] == 0:\n",
    "        return np.array([])\n",
    "    #print(matches_forward.shape)\n",
    "    #print(matches_reverse.shape)\n",
    "    matches = []\n",
    "    mr = list(matches_reverse[:,1])\n",
    "    for m in matches_forward:\n",
    "        if m[0] in mr:\n",
    "            matches.append(m)\n",
    "    matches = np.array(matches)\n",
    "    return matches\n",
    "#\"\"\"   \n",
    "matches = local_matcher.match(query_desc, neighbor_desc)\n",
    "#matches = double_matching(local_matcher, query_desc, neighbor_desc)\n",
    "print(matches.shape)\n",
    "matches_to_cv = lambda matches: [cv2.DMatch(_queryIdx=m[1], _trainIdx=m[0], _imgIdx=0, _distance=1.0) for m in matches]\n",
    "#matches_to_cv = lambda matches: [cv2.DMatch(_queryIdx=i, _trainIdx=m[0], _imgIdx=0, _distance=1.0) for i, m in enumerate(matches)]\n",
    "t = time.time() - t\n",
    "print('Matching took {:.1f} seconds\\nFound {} matches'.format(t, len(matches)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_stats = lambda x: (x.min(), np.median(x),x.mean(),x.max())\n",
    "import transforms3d.quaternions as txq"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if query_id >= num_query_images:\n",
    "    ## query\n",
    "    query_pos = colmap_image_to_pose(images[get_img_id(database_cursor, fake_query_path)])[:3,3]\n",
    "    query_path = path_to_queries[query_id].replace('data/AachenDayNight/images_upright/', '')\n",
    "    if 'Augmented' in query_path:\n",
    "        query_path = query_path.replace('data/AachenDayNight/AugmentedNightImages_high_res/', 'db/').replace('.png', '.jpg')\n",
    "    cm_query = camera_matrices[query_path]\n",
    "    camera_matrix_query = cm_query['cameraMatrix']\n",
    "    distortion_coeff_query = cm_query['rad_dist']\n",
    "    \n",
    "    ## neighbor\n",
    "    nbor_pos = images[abs(db_id)].tvec\n",
    "    nbor_pos = -txq.rotate_vector(nbor_pos, (images[abs(db_id)].qvec))\n",
    "    cm_nbor = camera_matrices[images[abs(db_id)].name]\n",
    "    camera_matrix_nbor = cm_nbor['cameraMatrix']\n",
    "    distortion_coeff_nbor = cm_nbor['rad_dist']\n",
    "    \n",
    "    print('Distance: {:.2f}m'.format(np.linalg.norm(query_pos-nbor_pos)))\n",
    "    query_q = images[get_img_id(database_cursor, fake_query_path)].qvec\n",
    "    nbor_q = images[abs(db_id)].qvec\n",
    "    \"\"\"\n",
    "    T = query_pos - nbor_pos\n",
    "    T = txq.rotate_vector(T, txq.qinverse(nbor_q))\n",
    "    tx, ty, tz = T\n",
    "    S = np.array([[0, -tz, ty],[tz, 0, -tx], [-ty, tx, 0]])\n",
    "    #print(S)\n",
    "    #diff * q1 = q2  --->  diff = q2 * inverse(q1)\n",
    "    rot = txq.qmult(nbor_q, txq.qinverse(query_q))\n",
    "    print('{:.2f}°'.format(quaternion_angular_error(query_q, nbor_q)))\n",
    "    #print('{:.2f}°'.format(quaternion_angular_error(nbor_q, txq.qmult(rot, query_q))))\n",
    "    R = Quaternion(list(rot)).rotation_matrix\n",
    "    print(R)\n",
    "    E = np.matmul(R, S)\n",
    "    print(E)\n",
    "    F = np.matmul(inverse_transpose(camera_matrix_nbor), np.matmul(E,inverse(camera_matrix_query)))\n",
    "    print(F)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(camera_matrix_query)\n",
    "default_camera_matrix = np.array([[1, 0, 800], [0, 1, 600], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "skew = lambda v: np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "#pose2fund = lambda K1, K2, R, t: np.linalg.inv(K2).T @ R @ K1.T @ skew((K1 @ R.T).dot(t))\n",
    "pose2fund = lambda KL, KR, R_L_R, W_L_R, L_R_R, T_W: np.linalg.inv(KL.T)@L_R_R@skew(R_L_R@W_L_R@T_W)@np.linalg.inv(KR)\n",
    "calc_epi_dist = lambda x1, X, x2: np.abs(np.diag(np.einsum('ij,kj->ik', x1, np.einsum('jl,ij->il', X, x2))))\n",
    "\n",
    "\n",
    "KL = camera_matrix_query\n",
    "KR = camera_matrix_nbor\n",
    "\n",
    "W_L_R = Quaternion(list(query_q)).rotation_matrix\n",
    "W_R_R = Quaternion(list(nbor_q)).rotation_matrix\n",
    "L_R_R = np.matmul(W_L_R.T, W_R_R)\n",
    "R_L_R = np.matmul(W_R_R.T, W_L_R)\n",
    "\n",
    "T_W = nbor_pos - query_pos\n",
    "T_L = W_L_R @ T_W\n",
    "T_R = W_L_R.T @ T_W\n",
    "\n",
    "#print(W_L_R @ np.array([1,1,1]))\n",
    "#print(W_R_R @ L_R_R.T @ np.array([1,1,1]))\n",
    "#print(txq.rotate_vector(np.array([1,0,0]), query_q))\n",
    "\n",
    "#F = pose2fund(KL, KR, R_L_R, T_W)\n",
    "F_L = pose2fund(KL, KR, R_L_R, W_L_R, L_R_R, T_W)\n",
    "F_R = pose2fund(KR, KL, L_R_R, W_R_R, R_L_R, T_W)\n",
    "#F = pose2fund(default_camera_matrix, default_camera_matrix, R_L_R, W_L_R, L_R_R, T_W)\n",
    "\n",
    "if matches.shape[0] > 0:\n",
    "    p1 = query_kpts_numpy[matches[:,0]]\n",
    "    p2 = neighbor_kpts_numpy[matches[:,1]]\n",
    "    pts1 = expand_homo_ones(p1, axis=1)\n",
    "    pts2 = expand_homo_ones(p2, axis=1)\n",
    "    e = np.dot(pts2, F_L)\n",
    "    e = e.dot(pts1.T)\n",
    "    e = np.diagonal(e)\n",
    "    #print(e.shape)\n",
    "    print('Measure of quality for fundamental matrix is {}; Should be zero if matches are correct'.format(e.mean()))\n",
    "    #ed = symmetric_epipolar_distance(p1, p2, F, sqrt=True) \n",
    "    #ed_L = calc_epi_dist(pts1, F_L, pts2)\n",
    "    #ed_R = calc_epi_dist(pts2, F_R, pts1)\n",
    "    ed_L = symmetric_epipolar_distance(p1, p2, F_L, sqrt=True)\n",
    "    ed_R = symmetric_epipolar_distance(p2, p1, F_R, sqrt=True)\n",
    "    stats_L = get_stats(ed_L)\n",
    "    print(stats_L)\n",
    "    stats_R = get_stats(ed_R)\n",
    "    print(stats_R)\n",
    "    thresh = 100\n",
    "    valid = np.logical_or(ed_L < thresh, ed_R < thresh)\n",
    "    print('Number of matches with ep distance < {:.1f}: {}/{}'.format(thresh, np.sum(valid), matches.shape[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"n_points = 2\n",
    "x1 = np.arange(3*n_points).reshape(n_points, 3)+1\n",
    "X = np.array([[1,0,1],[0,1,0],[1,0,1]])\n",
    "x2 = - (np.arange(3*n_points).reshape(n_points,3)+1)\n",
    "print(x1)\n",
    "ref_value = (x1[0].reshape(1,3)@X@x2[0].reshape(3,1))[0,0]\n",
    "print(ref_value)\n",
    "#np.einsum('ij,jl->il', x1, X).shape\n",
    "temp = np.einsum('jl,ij->il', X, x2)\n",
    "print(x1.shape)\n",
    "print(temp.shape)\n",
    "print(temp)\n",
    "print(calc_epi_dist(pts1, F, pts2))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_below_thresh = False\n",
    "matches_cv = matches_to_cv(matches[valid]) if only_below_thresh and matches.shape[0] > 0 else matches_to_cv(matches)\n",
    "img3 = np.empty((max(query_img.shape[0], neighbor_img.shape[0]), query_img.shape[1] + neighbor_img.shape[1], 3), dtype=np.uint8)\n",
    "cv2.drawMatches(neighbor_img_original,neighbor_kpts_cv,np.array(query_imgs_high_res[query_id]),query_kpts,matches_cv,outImg=img3,\n",
    "                matchColor=(0,255,0), singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "plt.figure(figsize=(15,15), dpi=300)\n",
    "plt.imshow(img3)\n",
    "plt.axis('off')\n",
    "if query_id >= num_query_images:\n",
    "    print(dataset_queries[query_id - num_query_images])\n",
    "    #plt.savefig('figures/few_inlier_matching_{}.png'.format(dataset_queries[query_id - num_query_images]), bbox_inches=\"tight\")\n",
    "    #plt.savefig('figures/local_matching_{}_{}_{}.png'.format(matching_type, method, dataset_queries[query_id - num_query_images]), bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def skew(x):\n",
    "    assert x.shape == (3,)\n",
    "    return np.array([[0, -x[2], x[1]], [x[2], 0, -x[0]], [-x[1], x[0], 0]])\n",
    "\n",
    "def inv_transpose(x):\n",
    "    assert x.shape == (3,3)\n",
    "    return np.linalg.inv(x.T)\n",
    "\n",
    "T_W = nbor_pos - query_pos\n",
    "W_L_R = Quaternion(list(query_q)).rotation_matrix\n",
    "W_R_R = Quaternion(list(nbor_q)).rotation_matrix\n",
    "print(W_L_R)\n",
    "print(W_R_R)\n",
    "T_L = np.matmul(W_L_R, T_W)\n",
    "L_R_R = np.matmul(W_L_R.T, W_R_R)\n",
    "S = skew(T_L)\n",
    "print(L_R_R)\n",
    "print(S)\n",
    "E = np.matmul(L_R_R, S)\n",
    "print(E)\n",
    "M_L = camera_matrix_query\n",
    "M_R = camera_matrix_nbor\n",
    "F = np.matmul(inv_transpose(M_R), np.matmul(E, np.linalg.inv(M_L)))\n",
    "print(F)\n",
    "\n",
    "F_qz = pose2fund(M_L, M_R, L_R_R, T_L)\n",
    "print(F_qz)\n",
    "F = F_qz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#if matches.shape[0] < 8:\n",
    "#    print('Too few matches for epipolar distance')\n",
    "#else:\n",
    "if True:\n",
    "    p1 = query_kpts_numpy[matches[:,0]]\n",
    "    p2 = neighbor_kpts_numpy[matches[:,1]]\n",
    "    f = cv2.findFundamentalMat(p1, p2, method=cv2.FM_RANSAC)\n",
    "    em = cv2.findEssentialMat(p1, p2, method=cv2.FM_RANSAC)\n",
    "    #print(f[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'Colmap' and query_id >= num_query_images:\n",
    "    oimg = images[get_img_id(database_cursor, 'db/'+path_to_queries[query_id].split('/')[-1].replace('.png', '.jpg'))]\n",
    "    nimg = images[image_ids[abs(indices[query_id,0])]]\n",
    "    #print(oimg)\n",
    "    #print(nimg)\n",
    "    \n",
    "    valid_o = oimg.point3D_ids > 0 \n",
    "    pt_ids_o = oimg.point3D_ids#[valid_o]\n",
    "    pt_ids_o = pt_ids_o[:pt_ids_o.shape[0]//2]\n",
    "\n",
    "    valid_n = nimg.point3D_ids > 0 \n",
    "    pt_ids_n = nimg.point3D_ids[valid_n]\n",
    "    \n",
    "    \n",
    "    #print(pt_ids_o.shape)\n",
    "    #print(pt_ids_n.shape)\n",
    "    \n",
    "    correct, incorrect = 0,0\n",
    "    incorrect_matches = []\n",
    "    for m1, m2 in matches:\n",
    "        if valid_o[m1]:\n",
    "            if pt_ids_o[m1] == pt_ids_n[m2]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                incorrect_matches.append([m1, m2])\n",
    "    incorrect_matches = matches_to_cv(np.array(incorrect_matches))\n",
    "    print('{} correct / {} incorrect matches ({})'.format(correct, incorrect, '{:.1f}%'.format(100.0*(correct / float(correct+incorrect))) if (correct+incorrect)> 0 else 'fail'))\n",
    "    img3 = np.empty((max(query_img.shape[0], neighbor_img.shape[0]), query_img.shape[1] + neighbor_img.shape[1], 3), dtype=np.uint8)\n",
    "    cv2.drawMatches(neighbor_img_original,neighbor_kpts_cv,np.array(query_imgs_high_res[query_id]),query_kpts,incorrect_matches,outImg=img3,matchColor=None, singlePointColor=(255, 255, 255), flags=2)# **draw_params)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title('Incorrectly matched')\n",
    "    plt.imshow(img3)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kpts, _ = get_kpts_desc(database_cursor, 1)\n",
    "print(test_kpts.shape)\n",
    "test_kpts2 = keypoints_from_colmap_db(database_cursor, 1)\n",
    "print(test_kpts2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(verify=None, local_method=method,augmentation=augmented)\n",
    "print(args)\n",
    "model = None\n",
    "if method == 'Superpoint':\n",
    "    model = extractor\n",
    "elif method == 'D2':\n",
    "    model = d2net\n",
    "else:\n",
    "    raise NotImplementedError('what is that')\n",
    "matched_pts_xyz, matched_keypoints, correct, incorrect = match_local(args, matching_type,\n",
    "                                                                     query_desc, query_kpts,\n",
    "                                                                     images, points3d, query_id,\n",
    "                                                                     cluster_query, database_cursor,\n",
    "                                                                     model,\n",
    "                                                                     matching.LocalMatcher(ratio_thresh, matching_type, True),\n",
    "                                                                     refilter=True, \n",
    "                                                                     double = True\n",
    "                                                                    )\n",
    "print('Found {} matches'.format(matched_pts_xyz.shape[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##settings\n",
    "extraction = method # 'colmap'\n",
    "\n",
    "if extraction == 'Colmap':\n",
    "    query_desc = query_desc_colmap\n",
    "elif extraction == 'Superpoint':\n",
    "    query_desc = query_desc_superpoint\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "matched_kpts_cv = []\n",
    "matched_pts = []\n",
    "correct, incorrect = 0,0\n",
    "if query_id >= num_query_images and extraction == 'colmap':\n",
    "    valid_o = oimg.point3D_ids > 0 \n",
    "    pt_ids_o = oimg.point3D_ids\n",
    "    pt_ids_o = pt_ids_o[:pt_ids_o.shape[0]//2]\n",
    "superpoint_cursor = sqlite3.connect('data/superpoint.db').cursor()\n",
    "print('Start')\n",
    "pt_ids_all = []\n",
    "data_descs = []\n",
    "matches_all = []\n",
    "for c in cluster_query:\n",
    "    for img in c:\n",
    "        if query_id > num_query_images and abs(img) == abs(query_img_id):\n",
    "            continue\n",
    "        #db_id = img # get_img_id_dataset(database_cursor, img)\n",
    "        valid = images[abs(img)].point3D_ids > 0 \n",
    "        pt_ids = images[abs(img)].point3D_ids[valid]\n",
    "        img_name = images[abs(img)].name\n",
    "        if extraction == 'Colmap':\n",
    "            data_kpts, data_desc = get_kpts_desc(database_cursor, abs(img))\n",
    "            #data_kpts_cv = kpts_to_cv(data_kpts[valid[:data_kpts.shape[0]]] - 0.5)\n",
    "            data_desc = data_desc[valid[:data_desc.shape[0]]]\n",
    "        elif extraction == 'Superpoint':\n",
    "            if img > 0:\n",
    "                path_to_img = 'data/AachenDayNight/images_upright/'+img_name\n",
    "            else:\n",
    "                path_to_img = 'data/AachenDayNight/AugmentedNightImages_high_res/'+img_name.replace('db/', '').replace('.jpg', '.png')\n",
    "            #print(img_name)\n",
    "            cv_img = cv2.imread(path_to_img, 0).astype(np.float32)/255.0\n",
    "            data_kpts = keypoints_from_colmap_db(database_cursor, int(abs(img)))\n",
    "            data_kpts = data_kpts[valid[:data_kpts.shape[0]]] - 0.5\n",
    "            _, data_desc, _ = extractor.run(cv_img, points=data_kpts)\n",
    "            data_desc = data_desc.T\n",
    "            #superpoint_cursor.execute('SELECT cols, desc FROM local_features WHERE image_id==?;',(int(db_id),))\n",
    "            #cols, desc = next(superpoint_cursor)\n",
    "            #data_desc = np.frombuffer(desc, dtype=np.float32).reshape(cols, 256)\n",
    "            #except:\n",
    "            #    print(db_id)\n",
    "            #    continue\n",
    "        else:\n",
    "            raise NotImplementedError('Method unknown')\n",
    "        ## prefilter matches\n",
    "        #matches = local_matcher.match(query_desc, data_desc)\n",
    "        matches = double_matching(local_matcher, query_desc, data_desc)\n",
    "        if matches.shape[0] > 1:\n",
    "            pt_ids_all.append(pt_ids[matches[:,1]])\n",
    "            data_descs.append(data_desc[matches[:,1]])\n",
    "\n",
    "            #matched_kpts_cv += [query_kpts[m[0]] for m in matches]\n",
    "            #matched_pts += [pt_ids[m[1]] for m in matches]\n",
    "pt_ids_all = np.concatenate(pt_ids_all)\n",
    "data_descs = np.vstack(data_descs)\n",
    "# refine matches\n",
    "matches = double_matching(local_matcher, query_desc, data_descs)\n",
    "#matches = local_matcher.match(query_desc, data_descs)\n",
    "if query_id >= 4 and extraction == 'colmap':\n",
    "    for m1, m2 in matches:\n",
    "        if valid_o[m1]:\n",
    "            if pt_ids_o[m1] == pt_ids_all[m2]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "        \n",
    "matched_kpts_cv = [query_kpts[m[0]] for m in matches]\n",
    "matched_pts = [pt_ids_all[m[1]] for m in matches]\n",
    "        \n",
    "\n",
    "t = time.time() - t\n",
    "superpoint_cursor.close()\n",
    "print('Total matching time: {:.2f} seconds'.format(t))\n",
    "if correct + incorrect > 0:\n",
    "    print('{} correct / {} incorrect matches ({:.1f}%)'.format(correct, incorrect, 100.0*(correct / float(correct+incorrect))))\n",
    "    \n",
    "matched_pts_xyz = np.stack([points3d[i].xyz for i in matched_pts])\n",
    "print(matched_pts_xyz.mean(axis=0))\n",
    "\n",
    "matched_keypoints = np.vstack([np.array([x.pt[0], x.pt[1]]) for x in matched_kpts_cv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_pts_xyz.shape)\n",
    "print(matched_keypoints.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = path_to_queries[query_id].replace('data/AachenDayNight/images_upright/', '')\n",
    "if 'Augmented' in query_path:\n",
    "    query_path = query_path.replace('data/AachenDayNight/AugmentedNightImages_high_res/', 'db/').replace('.png', '.jpg')\n",
    "cm = camera_matrices[query_path]\n",
    "camera_matrix = cm['cameraMatrix']\n",
    "distortion_coeff = cm['rad_dist']\n",
    "print(camera_matrix)\n",
    "print(distortion_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matched_pts_xyz.shape[0] < 4:\n",
    "    print('No matches found')\n",
    "    success = False\n",
    "    inliers = []\n",
    "else:\n",
    "    t = time.time()\n",
    "    dist_vec = np.array([distortion_coeff, 0, 0, 0])\n",
    "    success, R_vec, translation, inliers = cv2.solvePnPRansac(\n",
    "            matched_pts_xyz, matched_keypoints, camera_matrix, dist_vec,\n",
    "            iterationsCount=n_iter, reprojectionError=8.,\n",
    "            flags=cv2.SOLVEPNP_P3P)\n",
    "    t = time.time() - t\n",
    "    print('PnP RANSAC took %d seconds (%.2f/iteration)'%(t, float(t)/float(n_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    print('Successful matching')\n",
    "    print(inliers.shape)\n",
    "    print(R_vec)\n",
    "    print(translation)\n",
    "else:\n",
    "    print('Not succesful')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matched_pts_xyz[inliers].shape)\n",
    "print(matched_keypoints[inliers].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_inliers = 5\n",
    "\n",
    "\n",
    "if success:\n",
    "    inliers = inliers[:, 0] if len(inliers.shape) > 1 else inliers\n",
    "    num_inliers = len(inliers)\n",
    "    inlier_ratio = len(inliers) / len(matched_keypoints)\n",
    "    print('{} inliers ({:.1f}%)'.format(num_inliers, 100.0*inlier_ratio))\n",
    "    success &= num_inliers >= min_inliers\n",
    "\n",
    "    ret, R_vec, t = cv2.solvePnP(\n",
    "                matched_pts_xyz[inliers], matched_keypoints[inliers], camera_matrix,\n",
    "                dist_vec, rvec=R_vec, tvec=translation, useExtrinsicGuess=True,\n",
    "                flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    assert ret\n",
    "\n",
    "    query_T_w = np.eye(4)\n",
    "    query_T_w[:3, :3] = cv2.Rodrigues(R_vec)[0]\n",
    "    query_T_w[:3, 3] = t[:, 0]\n",
    "    w_T_query = np.linalg.inv(query_T_w)\n",
    "\n",
    "    #ret = LocResult(success, num_inliers, inlier_ratio, w_T_query)\n",
    "else:\n",
    "    if inliers is None:\n",
    "        num_inliers = 0\n",
    "        inliers = []\n",
    "    else:\n",
    "        num_inliers = len(inliers) if type(inliers) is list else len(inliers[:,0])\n",
    "print('Success') if success else print('Failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inlier_pts_img = cv2.drawKeypoints(np.array(query_imgs_high_res[query_id]),[query_kpts[i] for i in inliers],None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(inlier_pts_img)\n",
    "plt.axis('off')\n",
    "if query_id >= num_query_images:\n",
    "    print('Save image {}'.format(dataset_queries[query_id - num_query_images]))\n",
    "    #plt.savefig('figures/few_inliers_{}.png'.format(dataset_queries[query_id - num_query_images]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(w_T_query)\n",
    "if success:\n",
    "    print(w_T_query)\n",
    "    qvec_nvm = list(Quaternion(matrix=query_T_w)) # rotmat2qvec(w_T_query[:3,:3])\n",
    "    pos = -txq.rotate_vector(w_T_query[:3,3], qvec_nvm)\n",
    "    print('Calculated position: {}\\nOrientation: {}'.format(w_T_query[:3, 3], qvec_nvm))\n",
    "    print('Transformed position: {}'.format(pos))\n",
    "    if query_id >= num_query_images:\n",
    "        #pose_stats_filename = os.path.join('data/AachenDayNight/', 'pose_stats.txt')\n",
    "        #mean_t, std_t = np.loadtxt(pose_stats_filename)\n",
    "        #position = dataset[dataset_queries[query_id-4]][1][:3]\n",
    "        #position = position*std_t + mean_t\n",
    "        position = colmap_image_to_pose(images[get_img_id(database_cursor, fake_query_path)])[:3,3]\n",
    "        rotation = images[get_img_id(database_cursor, fake_query_path)].qvec\n",
    "\n",
    "        #query_T_w = np.linalg.inv(result.T)\n",
    "        #pos_nvm = query_T_w[:3, 3].tolist()\n",
    "\n",
    "        error_rot = quaternion_angular_error(rotation, qvec_nvm)\n",
    "        error = np.linalg.norm(position-w_T_query[:3,3])\n",
    "        error_str = '%.1f m'%error if error > 1e-1 else '%.1f cm'%(100.0*error)\n",
    "        print('Groundtruth: \\t%s \\nError transl.: \\t%s\\nAngular error: \\t%.2f°'%(str(position), error_str, error_rot))\n",
    "else:\n",
    "    print('No success')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#helped = [True, False, False, True, False, True, False, False, False, False, True, False, True, False, True, False, False,False]\n",
    "#print('{:.1f}%'.format(np.sum(helped)/len(helped)*100))\n",
    "other_nums = [2699,  2705,  2670,  2322,  2885,  3802,  4032,  4026,  421,  1228,  2504,  2580,  572,  4050,  4051,  4048,  1926]\n",
    "print('In old other but not new: ', end='')\n",
    "for i in dataset_queries:\n",
    "    if i not in other_nums:\n",
    "        print('{}, '.format(i), end='')\n",
    "print('\\nIn new other but not old: ', end='')\n",
    "for i in other_nums:\n",
    "    if i not in dataset_queries:\n",
    "        print('{}, '.format(i), end='')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfnet-pytorch",
   "language": "python",
   "name": "hfnet-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
